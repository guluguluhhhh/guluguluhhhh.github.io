[{"categories":null,"content":"进程、线程、协程 线程是CPU调度的最小单位, 进程是资源分配的最小单位。对于线程和进程，我们可以这么理解： 当进程只有一个线程时，可以认为进程就等于线程。 当进程拥有多个线程时，这些线程会共享相同的虚拟内存和全局变量等资源。这些资源在上下文切换时是不需要修改的。 线程也有自己的私有数据，比如栈和寄存器等，这些在上下文切换时也是需要保存的。 协程作为用户态线程，也是轻量级的线程，用来解决高并发场景下线程切换的资源开销。协程跟线程是有区别的 线程/进程是内核进行调度，有 CPU 时间片的概念，进行 抢占式调度（有多种调度算法） 协程对内核是透明的，也就是系统并不知道有协程的存在，是完全由用户自己的程序进行调度的，因为是由用户程序自己控制，那么就很难像抢占式调度那样做到强制的 CPU 控制权切换到其他进程/线程，通常只能进行 协作式调度，需要协程自己主动把控制权转让出去之后，其他协程才能被执行到。 GO协程的设计与原理 ","date":"2025-06-27","objectID":"/coroutine/:0:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":null,"content":"GMP模型 G（Goroutine）：用户态协程，逻辑上的计算单元 M（Machine/OS Thread）：绑定操作系统线程，实际执行G P（Processor）：调度器抽象，用于调度G给M ","date":"2025-06-27","objectID":"/coroutine/:1:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":null,"content":"go func ‘’‘go func main() { for i := 0; i \u003c 10; i++ { go func() { fmt.Print(i) }() } time.Sleep(1 * time.Second) } ’‘’ 上述代码的运行结果为： 这段代码中开启了十个线程，可以看到并不是按顺序打印且每次不一样，这是因为这十个线程的调度时间并不固定，只有等到被调度执行的时候才会实际打印。 每个我们开启的协程都是一个计算任务，这些任务会被提交给go的runtime。若有多个计算任务，这些计算任务会被先暂存起来，一般的做法是放到内存的队列中等待被执行。 消费端则是一个go runtime维护的一个调度循环。调度循环简单来说，就是不断从队列中消费计算任务并执行。这里本质上就是一个生产者-消费者模型，实现了用户任务与调度器的解耦。 ","date":"2025-06-27","objectID":"/coroutine/:2:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":null,"content":"调度策略 为了避免多个M访问同一个队列，我们把全局队列分为多个多个本地队列，这个本地队列由P来管理。这样一来，每个M只需要去先找到一个P结构，和P结构绑定，然后执行P本地队列里的G。 上图中各个模块的作用如下： 全局队列：存放等待运行G P的本地队列：和全局队列类似，存放的也是等待运行的G，存放数量上限256个。新建G时，G优先加入到P的本地队列，如果队列满了，则会把本地队列中的一半G移动到全局队列 P列表：所有的P都在程序启动时创建，保存在数组中，最多有GOMAXPROCS个，可通过runtime.GOMAXPROCS(N)修改，N表示设置的个数。 M：每个M代表一个内核线程，操作系统调度器负责把内核线程分配到CPU的核心上执行。 **个人感觉GMP中的M就像线程池中的线程，Go runtime会动态创建和复用M，避免频繁线程切换或销毁，提升并发性能。**区别在于线程池中的线程在等待IO的时候会阻塞，但协程模型中，这个线程会在调度器的调度下运行另一个任务，IO完成后再唤醒，且任务切换的开销远小于切换线程。 调度策略： 调度器核心思想是尽可能避免频繁的创建、销毁线程，对线程进行复用以提高效率。 work stealing机制（窃取式）：当本线程无G可运行时，从其他线程绑定的P窃取G，而不是直接销毁线程。 hand off机制：当本线程M1因为G进行的系统调用阻塞时，线程释放绑定的P，把P转移给其他空闲的M0执行。 抢占：一个goroutine最多占用CPU10ms，防止其他goroutine等待太久得不到执行被“饿死”。 优势： goroutine是用户态线程，其创建和切换都在用户代码中完成而无需进入操作系统内核，所以其开销要远远小于系统线程的创建和切换； goroutine启动时默认栈大小只有2k，这在多数情况下已经够用了，即使不够用，goroutine的栈也会自动扩大，同时，如果栈太大了过于浪费它还能自动收缩，这样既没有栈溢出的风险，也不会造成栈内存空间的大量浪费。 rust协程：tokio 通常，无栈协程在内存空间和协程上下文切换的效率更高。rust中的协程可以看作一个状态机结构体Future，一般放在堆上，并作为任务被推入一个异步任务队列等待调度器线程来调用。 ","date":"2025-06-27","objectID":"/coroutine/:3:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":null,"content":"协程运行机制 在rust中，async fn用来定义一个异步函数，被编译成状态机结构体Future，tokio::spawn会把Future推入到调度器的任务队列，实际线程从队列中取任务，调用Future::poll方法，返回Poll::Ready则任务完成，如果是Poll::Pending，就说明遇到了 .await 的阻塞点，注册 waker，挂起当前任务。 阻塞事件完成后，比如网络收到了响应、文件读取完成、锁可用等，Waker唤醒任务。被唤醒的任务再次 poll()，直到最终完成。 状态机的转换过程如下图： ","date":"2025-06-27","objectID":"/coroutine/:4:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":null,"content":"调度器 Tokio采用多线程、work-stealing调度策略： 每个线程都有一个本地任务队列； 新任务随机分配到某个线程； 空闲线程可以从其他线程“偷”任务； 每个任务的 Future 是 Send + ‘static，所以可以在不同线程间移动执行。 ","date":"2025-06-27","objectID":"/coroutine/:5:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":null,"content":"适用场景 I/O密集型 ","date":"2025-06-27","objectID":"/coroutine/:6:0","tags":["runtime","asynchronous","concurrency"],"title":"coroutine","uri":"/coroutine/"},{"categories":["Paper"],"content":"摘要 传统的主从架构（Primary- secondary）数据库的写入能力有限，因为其使用单个主节点来写入，从节点只是同步数据。所以一些系统采用无共享架构（shared-nothing），实现了可扩展的多主节点集群，但带来了分布式事务的开销和性能损失。最近，出现基于共享存储（shared-stroage）的多主节点云原生数据库，存算分离，避免了分布式事务，但在高冲突场景下仍表现不佳，原因有冲突解决、数据融合等。 PolarDB-MP是一种共享内存-存储架构（shared memory and storage），将内存和存储从计算节点中解耦出来。每个节点可以访问所有数据，事务可以在单个节点进行而无需进行分布式事务。 其核心是建立在共享内存上的Polar Multi-Primary Fusion Server (PMFS)，PMFS的三大功能是：Transaction Fusion、Buffer Fusion、Lock Fussion，提供全局的事务、缓存和锁管理，这些机制都基于 RDMA（远程直接内存访问），提供极低的通信延迟。 引入了 LLSN（Local Logical Sequence Number），用来协调不同节点生成的写前日志（WAL），配合“定制化的恢复策略”，可以实现高效、可靠的崩溃恢复。 ","date":"2025-06-14","objectID":"/polardb-mp/:0:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"1. 介绍 主从架构不仅有写入能力瓶颈，另外在主节点挂掉时，其中一个从节点会推举当作新的主节点，这一转换过程也带来一段耗时。因此，现在对多主节点架构数据库系统的需求日益增长，它可以提高写密集型操作的扩展性，特别是对于高并发场景，同时也拥有更高的高可用性，能做到故障自动切换无缝衔接。 两个最流行的多主节点架构是shared-nothing和shared-storage。 shared-nothing：（Spanner, DynamoDB, CockroachDB, PolarDB-X, Aurora Limitless, TiDB and OceanBase）整个数据库被划分成多个分区（partition），每个节点独立运行，只能访问属于它自己的那一部分数据。当有事务跨多个分区时，就必须依赖于跨分区的分布式事务机制（比如两阶段提交，2PC）来保持一致性，但这种机制通常会带来很大的额外开销。 shared-storage：（IBM pureScale, Oracle RAC, AWS Aurora Multi-Master (Aurora-MM)and Huawei Taurus-MM）本质上相反，所有数据对于所有集群节点都是可访问的。Aurora-MM使用**乐观并发控制（OCC）来处理写冲突，每个事务在开始时不加锁，先试着执行，提交时进行校验：如果有冲突，就直接回滚（abort），所以当写冲突频繁时，回滚率很高，导致性能急剧下降。Taurus-MM走的是悲观并发控制（PCC）**路线，对共享数据加锁，防止冲突发生，但依赖页存储和日志回放来保证缓存一致性，这种同步控制和数据同步带来巨大的开销。 PolarDB-MP继承了共享存储方式，且用的是PCC，与其他数据库用日志回放和页管理器来实现节点间的数据同步不一样的是，它使用分离式共享内存（shared-memory）来实现高效率的数据同步。随着各大云厂商越来越普遍地提供RDMA网络支持，PolarDB-MP整个设计深度融合了 RDMA 技术，以此来提升性能。 ","date":"2025-06-14","objectID":"/polardb-mp/:1:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"2. 背景 ","date":"2025-06-14","objectID":"/polardb-mp/:2:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"2.1 单主节点（Single-Primary）云原生数据库 如今许多云原生数据库基于主从架构，如图。主从架构通常包含一个主节点来承担读和写请求，一个或者多个从节点只负责读请求。每个节点是一个完整的数据库实例，然而，相对于传统的完整数据库，一个显著特征是使用了分离式共享存储。共享存储保证了容错性和一致性，增加节点不需要增加额外的存储空间，与传统的主从架构中每个节点维护自己的存储不同。 缺点： 写密集型场景下存在严重挑战。写操作都集中在主节点，无法并行写入，成为瓶颈。从节点只是备份或用于读请求，资源利用不充分。 这类架构无法通过加节点（scale out）来提升写性能，因为所有写操作只能走 primary，增加再多从节点也帮不上忙，想要提升写能力，只能加大单机资源（scale up）。 但 scale up 又受限于机器本身的资源（CPU、内存等）。云服务商通常会“榨干”一台机器上所有资源，留给你的增长空间有限。就算迁移到更强的机器，也涉及“重启迁移”，会造成停机。 主从架构的另一个严重问题是故障转移慢：primary 崩了，secondary 须切换为新主 → 需要时间 → 会导致短暂不可用。 ","date":"2025-06-14","objectID":"/polardb-mp/:2:1","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"2.2 Shared-nothing 架构 shared-nothing 是一种常见的“横向扩展（scaling out）”架构，被广泛应用于分布式计算和分布式数据库。 避免节点之间争抢资源（资源竞争），消除单点故障。数据会按照某种方式被切分（partition）到不同节点，每个节点只访问自己的数据分区。每个节点可以并发处理自己的数据，不需要协调资源，大幅提升可扩展性。 缺点： 一旦有事务涉及多个分区（跨节点），就必须使用分布式事务协议（如两阶段提交）来保证 ACID。但要在多个节点间高效同步，保证事务原子性/一致性等ACID特性，是一件极具挑战性的事情。 系统扩容/缩容时，常常需要重新分区数据（repartitioning），重新分区通常意味着大量数据迁移、系统中断甚至性能抖动。 ","date":"2025-06-14","objectID":"/polardb-mp/:2:2","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"2.3 Shared-storage 架构 在 shared-storage 架构中，每个节点都可以读写整个数据库，所以需要全局协调机制 来控制事务执行顺序和数据一致性。通过全局锁管理器控制不同节点对相同数据的访问顺序，避免写冲突，但增加通信和同步成本。通过集中式时间戳服务给每个事务分配唯一时间戳，实现全局可序性。 传统系统的问题（如 Oracle RAC / IBM pureScale） 架构上依赖高性能专用硬件（例如 Infiniband、共享磁盘阵列）； 不适合云环境（弹性差、扩容慢）； Aurora-MM 的问题（乐观控制） 多节点同时改同一页 → 冲突 → abort 应用需感知并处理死锁/回滚 某些 benchmark 场景下多节点还不如单节点快 Taurus-MM 的问题（悲观控制） 需要维持 page/page+log coherence 当节点请求某页数据时，必须：拉取页面（page），拉取日志（log），应用日志生成最新页面，这引入了大量存储 I/O 和 CPU 开销。 ","date":"2025-06-14","objectID":"/polardb-mp/:3:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"2.4 MVCC 和 事务隔离 MVCC使得每个数据项的多个版本同时存储。这些版本对于一个事务的可见性取决于隔离级别，在快照隔离下，每个事务会看到数据库在其启动时的一个快照，也就是说，它读取到的所有数据都是事务开始时刻的状态，在事务执行期间不会变化，从而保证了数据视图的一致性。这依赖于MVCC，这种机制可以有效隔离读写操作：读操作不会被写阻塞，写也不会影响读。这是 SI 的一个显著优势。 在shared-storage架构的多主数据库中，使用MVCC时，一个核心难题是：如何判断某个数据版本是否对当前事务可见。在传统方式下，跨节点判断数据版本是否可见需依赖同步的全局事务信息，开销极大。PolarDB-MP 通过 “每个节点维护本地事务信息 + RDMA 跨节点共享” 的方式，避免了中心协调，实现了高效低延迟的 MVCC 可见性判断。 ","date":"2025-06-14","objectID":"/polardb-mp/:4:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"2.5 RDMA 一个多主节点数据库集群不可避免需要不同节点间的数据同步和并发控制，使得网络成为影响性能的重要因素。好在网络技术的进步和迭代，让网络不再是瓶颈。如阿里巴巴的一个核心基础组件RDMA，给PolarDB-MP的实现提供了支持。利用基于RDMA的分离式共享内存，在节点之间直接传输数据页。另外，锁管理器和事务消息页通过RDMA网络传输。 3. PolarDB-MP 概述 **Polar Multi-Primary Fusion Server (PMFS)**是建立在分离式共享内存架构之上，它使得所有节点可以平等访问内存中的共享数据，并发处理读写请求。PMFS关键功能是管理全局的事务同步和缓冲区一致性。 Transaction Fusion：全局事务处理，保障 ACID + MVCC + Snapshot Isolation 全局 Timestamp Oracle (TSO)：为所有事务提供统一的提交顺序，保证全局一致性。TSO为每个事务分配唯一的时间戳，以确定先后顺序。 事务可见性管理（Snapshot Isolation）：支持基于 MVCC 的快照隔离，每个节点维护自己的本地事务状态。借助 RDMA，其他节点可以快速访问这些事务状态，从而判断某个数据版本是否在当前事务的快照中可见。 Buffer Fusion：缓存的一致性维护。 实现了一个分布式缓存池（Distributed Buffer Pool，DBP） 所有节点可以向 DBP 推送数据（push），或从 DBP 拉取数据（pull） 借助 RDMA + 分离式共享内存实现高性能 Lock Fusion：多节点的并发控制核心，支持并发访问的数据一致性和事务隔离。 PLock（Page Lock）：页级锁，用于保证物理页在多节点间的一致性。 RLock（Row Lock）：行级锁，用于保障事务隔离（如实现两阶段锁协议 2PL）。 在多主节点环境中，每个节点都可以独立生成 redo log（即write-ahead log, WAL）。如果没有统一的日志顺序，恢复时就无法判断每个页面的变更顺序，容易出现重放顺序错误，导致恢复失败或数据不一致。逻辑日志序列号（Logical Log Sequential Number）的作用： 为不同节点产生的日志赋予 逻辑顺序 按照数据页粒度维护 redo log 的顺序 这确保了页面级的日志重放是按时间顺序的，从而保证恢复的正确性 4. 设计和实现 ","date":"2025-06-14","objectID":"/polardb-mp/:5:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"4.1 Transaction Fusion 当一个事务进行到提交阶段，会从TSO请求一个提交时间戳CTS，这个CTS是逻辑自增的，维护事务的顺序性。PolarDB-MP使用了一种去中心化的方法，把事务信息分散到所有节点。每个节点保留一小部份内存来存储本地事务信息表TIT，节点可以通过单边RDMA来获取其他节点的事务信息。 TIT在事务管理中发挥重要作用，它对每个事务维护了四个key： pointer：事务对象的指针 CTS：事务的提交时间戳 version：标识了的一个slot中的不同事务 ref：标志着是否有其他事务在等待这个事务释放锁 当一个事务在某节点上开始执行时，会分配一个逻辑自增的唯一ID（trx_id）。为了让所有节点能访问任何事务的元信息，PolarDB-MP 构造了一个全局唯一事务 ID：g_trx_id = (node_id, trx_id, slot_id, version) 通过这个ID，其他节点可以用 RDMA 远程访问目标节点上的TIT。每一行用户数据（row）增加了两个字段： g_trx_id：标明是哪个事务最后更新了这个行版本； CTS（commit timestamp）：标明该版本的提交时间； 在读取时，只需比对当前事务的 snapshot 与 row 的 g_trx_id和CTS，就能判断可见性，不需要中心协调器，不需要所有节点同步完整的事务表。一个事务提交时，若row在buffer中，会更新row的元数据中的CTS，若不在，CTS为默认值。 判断可见性： 如果 row 上已经写了 CTS，那很好，直接判断是否小于读视图里的时间戳即可。 否则，row 的 CTS 缺失，通过 g_trx_id 去 TIT 表里找。先查本地 TIT 表，如果没有就通过one-sided RDMA远程读该 node 的 TIT 表中的对应 slot。之后比较version 是否一致（否则可能误拿到别的事务的数据），如果 CTS 是有效时间戳 → 正常比较，如果不一致，说明原事务已经结束、row 肯定已经提交了，视为“谁都能看”。 TIT回收：TIT 是内存表，slot 数有限，多事务并发会导致很快用完所有slot。Transaction Fusion 计算全局最小视图（global min CTS），广播给所有节点，节点可以安全回收CTS \u003c global_min_CTS的slot。 TSO优化：为了减少读事务频繁获取全局CTS的开销，利用Lamport Timestamp重用机制：请求到达时，如果最近系统拿到了一个来自未来的CTS，就可以直接复用，而不需要重新fetch。（应该是有一个之前的事务fetch的过程中，这个事务到达了，然后才拿到CTS回来，这时就可以复用） ","date":"2025-06-14","objectID":"/polardb-mp/:6:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"4.2 Buffer fusion Buffer Fusion是为了解决多节点共享读写数据页时的一致性问题。在多主架构（multi-primary）中，每个节点都可以修改任意一页，因此需要一个机制来： 保证其它节点读到的是最新的页（可见性）； 避免多个节点各自读写同一页产生写冲突、脏读或覆盖； 不靠中央协调器完成这些事情（分布式无中心）。 Buffer Fusion 的核心机制包括: 每个节点有自己的 LBP（Local Buffer Pool），就是普通意义上的缓存页，但是每个页带了两个额外字段： valid: 当前页是不是有效（是否需要重新拉取最新） r_addr: 指向 DBP 中这个页的远程地址 所有 LBP 共同组成 DBP（Distributed Buffer Pool），DBP 中的页版本是“最新”的，任何节点都可以从中获取。 更新传播方式：Push-Based + Remote Invalidation。节点对某页进行修改后，在“合适的时机”把页 push 到 DBP，DBP 接收到新页后，会更新元数据（哪些节点有副本）利用这些节点 valid 字段的地址，通过 RDMA 把它们的副本标记为 invalid 页访问流程： LBP 命中且valid，直接读 LBP 命中但invalid用 r_addr 走一侧 RDMA 从 DBP 拉取页 LBP 未命中，发 RPC 到 Buffer Fusion 查询页状态： 如果 DBP 有页，Buffer Fusion 记录当前访问节点为“active node”（更新页的 metadata），返回页在 DBP 中的地址，访问节点使用 一侧 RDMA 去 DBP 拿页，并加载到自己的 LBP。 如果 DBP 也没这个页，从 Shared Storage（比如分布式存储系统）读，加载后注册到 DBP（上传），页通过 远程写入（remote write）方式写入DBP，LBP 也缓存一份。 ","date":"2025-06-14","objectID":"/polardb-mp/:7:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"4.3 Lock fusion 用PLock保持并发一致性，PLock是一种跨节点的页级互斥机制，确保不会有两个节点同时修改一个结构页。 RLock 是跨节点的全局行级排他锁，用于在多主节点场景中保障事务级一致性。仅支持 X-lock，通过写入行内字段和 Lock Fusion 的协助机制实现高效、低通信开销的 2PL。 ","date":"2025-06-14","objectID":"/polardb-mp/:8:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["Paper"],"content":"4.4 日志顺序和恢复 PolarDB-MP 继承了传统数据库中的 ARIES 风格日志系统。有Redo Log（预写日志）保证持久化，恢复提交事务修改；Undo Log用来回滚未提交的事务。每个节点本地维护日志，每个节点都有独立的 redo/undo log 文件，节点可以并行将日志写入存储系统，不需额外同步或加锁。事务提交前，必须先将 redo log 持久化 → WAL（Write-Ahead Logging）原则。 在 PolarDB-MP 的多主架构中，不同节点可以并发更新同一页，每个节点都会生成独立的 redo log。所以在恢复时，需要确保针对同一页的日志按写入顺序依次重放，而对不同页的日志，不需要统一排序，任意顺序都可。提出LLSN，不再强求全局日志排序（Global LSN），而是采用每页局部排序策略。 LLSN 的核心规则和同步机制： 每个节点本地维护一个 LLSN 计数器； 节点更新某页时，会将当前 LLSN 写入： 页 metadata 中； redo log 条目中； 如果节点访问的页的 LLSN 比自己本地 LLSN 更大，就更新自己的 LLSN； 每次生成新日志时，LSSN 自增，保证日志顺序性； 利用 PLock 机制 保证任意时刻只有一个节点能修改某页 → 避免 LLSN 竞争。 恢复：按 LLSN 顺序 replay 同一页的 redo log，避免加载所有日志文件、占用大量内存、做全量排序）。 分批次处理日志文件（chunk-based）； 每轮从每个节点读取一小段日志（chunk）； 在这些 chunks 中确定一个 LLSN_bound： 该值以下的日志肯定是“安全可以 replay 的”； 日志中 LLSN \u003c LLSN_bound → 立即应用； 日志中 LLSN ≥ LLSN_bound → 下一轮处理； ","date":"2025-06-14","objectID":"/polardb-mp/:9:0","tags":["Distributed Systems","database"],"title":"PolarDB MP","uri":"/polardb-mp/"},{"categories":["MIT6.5840"],"content":"3A 领导人选举 3A主要实现的是领导者选举和心跳（没有日志条目的AppendEntriesRPC）。 基本上根据论文的图2来定义结构体。Raft结构体加上electionTimeOut bool来标记某个服务器是否超时，超时未收到心跳需要发起新一轮选举。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:1:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"RequestVote RPC 上锁 如果args.Term小于自己的currentTerm，说明选举人的日志不是最新的，则拒绝。 如果args.Term等于自己的currentTerm且自己的votedFor已经投给了别人，也拒绝。 先判断args.Term大于自己的currentTerm，如果是就更新currentTerm，重置votedFor和state。 重置electionTimeOut为false，然后投票给args.CandidateId，并返回true。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:1:1","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"AppendEntries RPC 目前只需实现AppendEntries的一小部分 上锁 如果args.Term小于自己的currentTerm，说明选举人的日志不是最新的，则拒绝。 如果args.Term大于自己的currentTerm，更新currentTerm，重置votedFor。 成功，设置state为跟随者，重置electionTimeOut为false。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:1:2","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"make 这个函数中，除了初始化server的各个参数之外，开启两个goroutine，循环执行，一个给leader执行心跳，另一个判断如果超时了就重新开一轮选举。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:1:3","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"BroadcastHeartBeat 领导给所有peer发送心跳，循环时判断如果是自己就跳过。 启动一个goroutine，并发执行心跳的发送。 初始化args和reply，然后调用sendAppendEntries，发送日志为空的心跳。 此时先检查自己是否还是领导者，然后看如果回应是失败，检查reply.Term如果大于自己的currentTerm，说明自己过时了，直接转换为跟随者，更新currentTerm，重置votedFor和electionTimeOut，并返回。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:1:4","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"startElection 构建args和reply，然后调用sendRequestVote，启动goroutine发送请求投票的RPC。 检查自己是否还是候选者，之后如果reply.VoteGranted为true，计数加一，检查是否超过半数，过半了就转换为领导者，并返回。 如果reply.VoteGranted为false，检查reply.Term是否大于自己的currentTerm，如果是，说明自己不是最新的了，转换为跟随者，重置参数。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:1:5","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"3B 日志复制 3B主要实现的是日志复制，即领导者将日志复制给跟随者。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:2:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"ReequestVote RPC 这里在进行投票时，需要加上判断候选者的日志是否是最新的。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:2:1","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"AppendEntries RPC 在上一次的基础上，检查在log中args.PrevLogIndex位置的日志是否匹配任期。 删除prevLogIndex之后的日志条目，不是心跳则追加新日志条目。 更新commitIndex，然后把日志应用到状态机，即加入到applyCh for rf.lastApplied \u003c rf.commitIndex { i := rf.lastApplied + 1 rf.applyCh \u003c- ApplyMsg{ CommandValid: true, Command: rf.log[i].Command, CommandIndex: i, } rf.lastApplied = i } ","date":"2025-03-02","objectID":"/mit6.5840-raft/:2:2","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"Start 这个函数就是把用户的指令装成日志条目，append到领导者的日志中。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:2:3","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"BroadcastHeartBeat 遍历发送的时候，判断如果该peer日志不是最新的就加上entries让他添加，否则就发送空entries表示心跳。 成功的话，需要维护matchIndex和nextIndex。 失败的话，需要加上判断，若不是因为自己不够新导致的，那就是nextIndex不匹配，需要减1。 然后对当前任期的每个日志条目，统计每个跟随者的matchIndex大于等于该条目index的数量，如果大于半数，则更新commitIndex，并应用到状态机。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:2:4","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"startElection 选中领导者的时候，初始化所有nextIndex为当前日志长度，所有matchIndex为-1。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:2:5","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"3C 持久性 这里主要是实现persist()和readPersist()两个函数，然后在raft.go中服务器的当前任期、日志、投票人发生变动时调用，以持久化存储状态。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:3:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"persist() 使用labgob创建编码器，将当前服务器的当前任期、日志、投票人信息编码进去，按照注释中的流程将状态编码序列话，然后用persister存储。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:3:1","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"readPersist() 用labgob创建一个解码器，解码状态，并赋值给当前服务器的当前任期、日志、投票人。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:3:2","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["MIT6.5840"],"content":"优化 可以优化该协议以减少被拒绝的AppendEntriesRPC的数量。例如，当一个跟随者拒绝一个 AppendEntries 请求时，它可以包含冲突条目所属的任期以及该任期中它存储的第一个索引。通过这些信息，领导者可以将 nextIndex减小，从而跳过该任期内所有冲突的条目。这样，每个包含冲突条目的任期只需要一次AppendEntriesRPC，而不是每个条目都需要一次RPC。 ","date":"2025-03-02","objectID":"/mit6.5840-raft/:3:3","tags":["Distributed Systems","Consensus Algorithm"],"title":"Mit6.5840 Raft","uri":"/mit6.5840-raft/"},{"categories":["Paper"],"content":"摘要 Raft是一种用于管理复制日志的共识算法。它的结果等价与Paxos，并且效率相当，但它的结构更容易理解。为了增强可理解性，Raft将共识的关键要素（例如领导者选举、日志复制和安全性）分离开来，并强制执行更强的一致性要求，从而减少了需要考虑的状态数量。此外，Raft 还引入了一种新的集群成员变更机制，该机制通过使用重叠的多数派来保证安全性。 ","date":"2025-02-20","objectID":"/raft/:0:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"1 介绍 论文先讲Paxos太难理解了blabla，所以想要设计一个更让人容易接受的方法，也就是Raft。Raft与其他共识算法类似，但它有几个新特征： 更强大的领导者：领导力的形式更强，比如日志条目仅从领导者流向其他服务器，这简化了复制日志的管理。 领导者选举：使用随机定时器来选举领导者。这只在其他共识算法所需的心跳机制增加了很小的一部分，但是却可以简单而迅速地解决冲突。 集群成员变更：Raft的joint consensus方法通过在成员变更期间维护旧配置（参与算法的服务器集合）和新配置的多数派，确保了系统的高可用性和一致性，这种方法允许集群更换配置时继续正常操作，无需中断。 ","date":"2025-02-20","objectID":"/raft/:1:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"2 复制状态机 在这种方法中，一组服务器上的状态机计算相同状态的完全一致副本，即使某些服务器宕机，这些状态机仍能继续运行。状态机是用来解决分布式系统中的故障容忍问题，比如管理领导者选举，并存储那些可以拯救领导者崩溃的信息。复制状态机的例子包括Chubby和ZooKeeper。 复制状态机的实现就是用图1中的复制日志，每个服务器存储一个包含一些列指令的日志，然后所有状态机按顺序执行，每个都有相同顺序的输出结果。 保持复制日志的一致性是共识算法的职责。服务器上共识模块接收到客户的命令后添加到日志中，它与其他服务器的共识模块通信，确保即使某些服务器发生故障，每个日志最终都包含相同顺序的请求，因此这些服务器看起来形成了一台单一、可靠的状态机。 实际系统的共识算法有以下特性： 安全性：在所有非拜占庭条件（没有叛徒）下不会返回错误结果。 可用性：只要大多数服务器正常，就可以保持功能可用。 不依赖事件来确保日志一致性，故障的时钟和极端消息延迟只会导致可用性问题。 一条指令可以在集群的大多数节点对一轮RPC响应后立即完成，少数慢的节点不影响性能。 ","date":"2025-02-20","objectID":"/raft/:2:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"3 Paxos有什么问题？ 作者再次吐槽Paxos尤其难以理解，blabla Paxos架构不适合构建实际系统，主要由于单决策分解（我就不尝试理解了）。 ","date":"2025-02-20","objectID":"/raft/:3:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"4 为了更好理解而设计 基于对Paxos的槽点，作者选择了两种普遍使用的技术： 问题分解。将问题划分为可以相对独立解决、解释和理解的单独部分。 通过减少需要考虑的状态数量来简化状态空间，使系统更加一致，并尽可能消除不确定性。 ","date":"2025-02-20","objectID":"/raft/:4:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5 Raft共识算法 Raft是一种管理复制日志（如第二节讲述的形式）的算法。图2以浓缩形式总结了该算法，图3列出了该算法的关键属性。 Raft实现共识，先第一步选举一个领导者，然后将管理复制日志的全部责任交给它。领导者从客户那边接收日志，复制他们到其他服务器，然后告知这些服务器何时才能安全的把日志条目应用到状态机。拥有一个领导者可以简化复制日志的管理，比如领导者可以在不咨询其他服务器的情况下决定新条目在日志中的存放位置，并且数据以简单的方式从领导者流向其他服务器。如果领导者发生故障或与其余服务器断开连接，则选举一个新的领导者。 根据领导者方法，Raft把共识问题拆解为三个相对独立的子问题： 领导者选举 日志复制 安全性 ","date":"2025-02-20","objectID":"/raft/:5:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5.1 Raft基础知识 一个Raft集群一般有五个服务器，容错能力是两台崩溃。任何时刻，每个服务器的状态是这三种之一：领导者、跟随者、候选者。正常情况下一个是领导者，其他服务器都是跟随者。跟随者是被动的：它们只会响应请求。领导者处理所有客户请求（如果客户联系跟随者，跟随者会重定向给领导者）。候选者是用来选举新领导者的。图4描述了状态和它们的转换。 Raft把时间划分为任意长度的任期，如图5。每个任期从一次选举开始，一个或多个候选者尝试成为领导者，具体见5.2。某些情况下，选举可能导致票选分散，这个任期会没有领导者，马上一个新的选举会开始，Raft会保证每个任期只有最多一个领导者。 任期在Raft中充当逻辑是中，并允许服务器检测过时的信息，例如旧领导者。每个服务器存储了一个当前任期变号，随时间单调增长。服务器通信时会交换该值，若是发现该值不一样，则会更新为更大的那个。如果领导者或者候选者发现自己的当前任期编号过期了，就会立即恢复为跟随者状态。如果服务器收到带有过期任期编号的请求，会拒绝该请求。 Raft服务器通信使用远程程序调用（RPCs），基本的共识算法只需要两种RPCs。RequestVote RPCs由候选者在选举时发起的，还有AppendEntries RPCs是由领导者发起的，用来复制日志条目和提供一种心跳的形式（5.3）。服务器在一段时间没收到回应时会重发RPCs，它们并行发送RPCs以提高性能。 ","date":"2025-02-20","objectID":"/raft/:5:1","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5.2 领导者选举 Raft使用一种心跳机制来触发领导者选举。服务器刚开始都是跟随者，直到收到了来自领导者或者候选者的RPCs。领导者会定期发送心跳（不携带日志条目的AppendEntries RPCs）给所有跟随者来维持它的权威。如果一个跟随者一段时间没有收到通信，也叫做选举超时时间，那么它就会认为没有有效领导者并发起一个新的选举。 用来开启选举，一个跟随者增加当前任期编号并转换为候选者。然后它为自己投票，并行发送RequestVote RPCs给集群中所有其他服务器。一个候选者保持它的状态直到以下三种情况之一发生： （a）它赢得选举。一器一票，在同一任期获得大多数服务器选票就赢得选举。一旦候选者赢得选举，它就成为领导者并发送心跳信息确立权威，防止新的选举发生。 （b）另一个服务器赢得选举。等待投票时，候选者可能收到心跳信息，如果包含在RPC中的任期编号至少和自己一样大，他就认为是合法的新领导者并切换回跟随者。如果RPC中的任期编号比自己小，该候选者就拒绝该RPC，保持候选者状态。 （c）选举超时。如果同时有很多候选者，投票可能很分散，导致没有候选者获得大多数票。当这种情况发生时，每个候选者都会超时，通过增加任期编号并发起新一轮的RequestVote RPC来启动新的选举。然而，如果没有额外的措施，选票分散的情况可能会无限重复下去。 Raft使用随机化的选举超时时间来确保选票分散的情况很少发生并能被快速解决，这样大部分情况下只有一个服务器会超时。刚开始的时候，第一个超时的服务器会发起选举并赢得选举，发送心跳，在其他服务器超时之前确立领导地位。 ","date":"2025-02-20","objectID":"/raft/:5:2","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5.3 日志复制 领导者一经确定就开始处理客户端请求。每个客户请求包含一个状态机执行的命令，领导者把这个命令加入到自己的日志下面作为新条目，然后并行发送AppendEntries RPCs给其他服务器来复制条目。当条目被安全复制后，领导者把条目应用到自己的状态机，返回结果给客户端。如果跟随者发生崩溃、运行缓慢或者数据包丢失，领导者会无限重试AppendEntries RPCs，直到条目被复制成功。 日志的组织如图6，每个日志条目都存储一个状态机命令加上收到条目时的任期编号。在日志条目中的任期编号用来检测日志间的非一致性，确保图3中的一些特性。每个日志条目也有一个整型索引来标志他在日志中的位置。 领导者决定何时可以将日志条目安全地应用到状态机上，这样的条目被称为已提交committed。Raft保证已提交的条目是持久化的，并且最终会被所有可用的状态机执行。当创建该条目的领导者已经在大多数服务器上复制了该条目时，日志条目就被视为已提交，这也会提交所有先前的条目。领导者会跟踪其已知最高的已提交索引，并在未来的AppendEntries RPC（包括心跳消息中包含该索引，以便其他服务器最终也能获知这一信息。一旦跟随者得知某个日志条目已被提交，它就会按照日志顺序将其应用到本地状态机上。 设计这个日志机制是为了在不同的服务器上保持高度的一致性，简化系统操作、更加可预测而且是确保安全性的关键部分。Raft维护了以下属性，这些属性共同构成了图3中的日志匹配属性（Log Matching Property）： 如果两个不同日志中的条目具有相同的索引和任期号，那么它们存储的是相同的命令。（由于领导者在给定的任期内，对于一个给定的日志最多创建一个条目，且条目在日志上的位置永远不会改变） 如果两个不同日志中的条目具有相同的索引和任期号，那么这两个日志在所有先前的条目上都是完全相同的。（因为领导者发送RPC时会有一致性检查，跟随者接收新条目的RPC时会检查日志中是否找得到前一条目，否则会拒绝新条目） 因此，每当AppendEntries RPC成功返回时，领导者就知道跟随者的日志在新条目范围内与自身的日志完全一致了。 然而，领导者崩溃可能导致日志不一致，这些不一致可能会在一系列领导者和跟随者崩溃过程中加剧。图7展示了不一致性的各种方式。目录的缺失条目或多余条目可能跨越多个任期。 Raft中，领导者通过强制跟随者的日志复制自己的日志来处理不一致性，即被领导者覆盖。5.4节将展示，当结合一个额外的限制时，这样子是安全的。 为了做到这一覆盖，领导者必须找到两者最后保持一致的那个条目，然后把该条目之后的所有条目发给跟随者。领导者为每个跟随者维护一个nextIndex，表示将发送给它的下一个条目索引。每次发送AppendEntries RPC时执行一致性检查，如果不一致则会被拒绝，领导者会递减nextIndex并重试AppendEntries RPC直到一致为止。之后追加的条目会覆盖原先任何冲突的条目。 通过这种机制，领导者在上任时不需要采取任何特殊措施来恢复日志的一致性。它只需开始正常操作，日志会在AppendEntries一致性检查失败时自动收敛。领导者从不覆盖或删除其自身日志中的条目（如图3中的 Leader Append-Only Property 所述）。 ","date":"2025-02-20","objectID":"/raft/:5:3","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5.4 安全性 然而截止目前描述的来看，这些机制并不能确保每个状态机以正确的顺序执行相同的命令。比如一个跟随者可能在领导者提交了几个日志目录后变得不可用，然后它可能被选举为领导者并用新条目覆盖这些已提交的条目。导致不同状态机可能执行不同序列。 5.4.1 选举限制 在任何基于领导者的共识算法中，领导者都要最终存储全部已提交的日志条目，这些算法开发了额外的机制来找出那些丢失条目并传递给新领导。但可惜这带来了巨大的额外复杂度。Raft用了一种更简单的方式，它保证从选举开始，先前领导者的所有日志条目都会出现在新领导者上，不需要额外的传递。这意味着日志条目单向流动，从领导者到跟随者，领导者永远不会重写已有的日志。 为了得到选举，候选者必须按顺序联系大部分服务器，意味着每个已提交的条目必须出现在它们其中一个服务器上。如果候选者的日志已经是这大部分服务器中最新的了，那么就代表它已经包含了所有已提交条目。RequestVote RPC实现了这一限制：RPC包含了候选者的日志信息，投票者若发现自己日志比它新，就会拒绝该投票。 5.4.2 提交前一个任期的条目 如果领导者在提交条目之前就崩溃了，那么未来的领导者会尝试完成条目复制。然而，领导者不能认为只要之前的条目被大多服务器存储，这个条目就是已提交了。图8描述了一个情况，就是一个旧的日志条目已经被大多服务器存储，但仍然可以被未来的领导覆盖。 为了消除如图8中所示的问题，Raft从不通过计算副本数量来提交之前任期的日志条目。只有当前领导者任期内的日志条目可以通过计算副本数量来确认提交。一旦当前任期的某个日志条目以这种方式被确认提交，那么所有之前的日志条目都会因为日志匹配属性（Log Matching Property）而被间接提交，实现图8中（e）的效果。 5.4.3 安全性论证 论文中Raft协议的安全性保证是指：一旦某个节点将某一日志条目应用于自己的状态机，则其他节点不可能在此日志索上应用其他不同的日志条目。 李龙海老师的更简单一些的解释：一旦某个节点将日志中的某条指令应用于自己的状态机之后，它确信其他节点（如果还活着）迟早会将同样的指令（在日志中相同的位置）应用到它们自己的状态机。 思考：为什么Leader收到半数以上Follower已经把某个日志存储到本地日志的ACK消息之后，就可以通知大家可以安全地应用（commit）该日志指令了？ 答：伏地魔思想（李老师说伏地魔死后把自己的灵魂放入七个圣器遍布世界各地，被后人捡起来就灵魂附体，得以复活。。。）。其实这里就是指如果大多数跟随者已经复制了Leader的日志，然后即使此时Leader崩溃，也会有其中一个已经复制日志了的节点来竞选成为新Leader，继承老Leader的意志。 ","date":"2025-02-20","objectID":"/raft/:5:4","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5.5 跟随者和候选者崩溃 跟随者和候选者的崩溃要容易的多，如果他们崩溃了，那么未来发送过来的RequestVote RPCs和AppendEntries RPCs就会失败，Raft处理失败的办法是无限重试。如果崩溃的服务器重启了，RPC就会成功完成。Raft的RPCs是幂等（幂等性意味着多次执行同一操作与执行一次该操作的效果相同）的，因此不会造成问题。例如，如果一个跟随者接收到一条AppendEntries请求，而该请求中包含的日志条目已经存在于其日志中，那么它会忽略这些在新请求中的重复条目。 ","date":"2025-02-20","objectID":"/raft/:5:5","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"5.6 时序和可用性 我们对Raft的要求之一是安全性不能依赖于时序，系统不能仅仅因为某件事情发生的比预期快或慢产生错误结果。然而，可用性不可避免的依赖于时序。例如，如果消息交换所需要的时间超过了服务器崩溃之间能容忍的时间，那么候选者就没有足够时间来赢得选举，如果没有一个稳定的领导者，Raft就无法正常运行。 在Raf 中，领导者选举是最与时序密切相关的一部分。只要系统满足以下时序要求，Raft就能够成功选举并维持一个稳定的领导者： broadcastTime « electionTimeout « MTBF(单个服务器失败之间的平均时间) ","date":"2025-02-20","objectID":"/raft/:5:6","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"6 集群成员更换 有时候集群配置需要改变，比如替换失败的服务器或者改变复制的程度。且存在任何手动的步骤都会带来操作出错的风险，所以Raft将自动化配置变更纳入共识算法中。 为了配置更换机制的安全，不允许存在某个任期有同时两个领导者存在的可能。然而，任何服务器的交换方法都不安全，不可能一次性交换所有服务器，因此在转换过程中，集群有可能分裂为两个独立的多数派。 为保证安全，配置更换必须采用两阶段的方法。在Raft中，集群首先切换到我们称为 联合共识（joint consensus）的过渡配置；一旦联合共识被提交，系统随后切换到新配置。联合共识将旧配置和新配置结合起来： 日志条目被复制到两个配置的所有服务器。 两个配置中的任何一个服务器都可以成为领导者。 共识（针对选举和条目提交）需要分别从新旧配置中独立的获得多数同意。 集群配置的保存和交换是通过复制日志中的特殊条目实现的，图11描述了配置改变的过程。其中没有任何时刻是新旧集群同时做单边决策的，保证了安全性。 重新配置过程中还要解决三个问题： 新服务器可能最初不存储任何日志条目。这种情况的服务器加入时，可能需要相当长的时间才能赶上其他节点，这段时间可能无法提交新的日志条目。为避免可用性的中断，Raft在配置变更之前引入了一个额外阶段，新服务器以非投票者加入集群，领导者向他们复制日志条目，但他们计入多数派，直到赶上其他节点。 集群当前的领导者可能不在新的配置中。在这种情况下，一旦领导者提交了Cnew日志条目，它就会退位（返回到跟随者状态）。这意味着在提交 Cnew 的过程中，领导者将管理一个不包含自身的集群：它会继续复制日志条目，但不会将自己计入多数派中。领导者的切换发生在Cnew被提交时，因为这是新配置能够独立运行的第一个时刻（此时总是可以从Cnew中选出一个领导者）。在此之前，可能只有Cold中的服务器能够被选为领导者。 被移除的服务器（即不在Cnew中的服务器）可能会干扰集群的正常运行。这些服务器将不会接收到心跳消息，因此它们会因超时而启动新的选举。随后，它们会发送带有新任期号的 RequestVote RPC，这将导致当前领导者退回到跟随者状态。最终虽然会选出一个新的领导者，但被移除的服务器会再次超时，这一过程会不断重复。为了解决这个问题，服务器在认为当前领导者存在时会忽略 RequestVote RPC 请求。具体来说，如果服务器在从当前领导者接收到消息后的最小选举超时时间内收到了 RequestVote RPC，它不会更新自己的任期号或投出选票 ","date":"2025-02-20","objectID":"/raft/:6:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"7 日志压缩 在正常运行期间，Raft的日志会不断增长以纳入更多的客户端请求。但实际中需要某些机制来丢弃过时信息。快照技术是最简单的日志压缩方法。将整个当前系统的状态写入到稳定存储介质的快照中，然后日志中到该点的内容全都被删除。 图12展示了Raft快照的基本思想。每个服务器独立的进行快照，只涵盖日志中已提交的条目。大部分工作是状态机把当前状态写入快照。 尽管正常情况下服务器各自独立使用快照，但领导者还是要偶尔向拖后腿的跟随者发送快照，这会发生在领导者已经删除了下一条将要发送给跟随者的条目时。这种情况很少出现，但对于一些极慢的跟随者和刚加入的服务器，这是一种方式可以让他们跟上队伍。 如图13，领导者会发送一个叫InstallSnapshot的RPC给拖后腿的跟随者。当跟随者收到这个RPC时，一般会丢弃整个日志，因为日志中的所有内容都会被快照取代。 还有一种方法就是可以只允许领导者创建快照，然后发送给跟随者。但这会带来两个缺点：第一，发送快照给每个跟随者将浪费网络带宽，每个跟随者已经具备自己快照的能力且本地快照往往比通过网络接收一个快照开销更小；第二，领导者的实现更加复杂，比如你发送的同时要携带新的日志条目，避免阻塞新的客户需求。 然后还有两个问题会影响快照性能： 服务器必须决定何时生成快照。过于频繁会浪费磁盘带宽和能量，过于稀疏会耗尽存储容量，且重启时增加重放日志的时间。一种简单的策略是当日志达到固定字节大小时生成快照。如果将该大小设置为显著大于预期快照的大小，那么快照生成对磁盘带宽的开销将会很小。 写入快照可能需要很长时间，而我们不希望这会延迟正常操作。解决方案时写时复制技术，以便新的更新可以被接受且不影响正在写入的快照。例如，使用函数式数据结构构建的状态机天然支持这一特性。或者，也可以利用操作系统提供的写时复制支持（例如，Linux上的fork），来创建整个状态机的内存快照。 ","date":"2025-02-20","objectID":"/raft/:7:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["Paper"],"content":"8 客户端交互 这一节讲的是客户如何与Raft交互，包括客户如何找到集群领导者，以及Raft如何支持线性化语义。 客户只给领导者发送请求，一开始客户先随机选一个服务器，如果不是领导者，这个服务器就会拒绝请求然后提供最近的领导者信息。如果领导者崩溃，客户请求会超时，再次尝试还是随机的服务器。 我们的目标是让Raft实现线性化语义。然而，Raft可能会多次执行一个命令，比如领导者在提交条目之后返回给客户之前崩溃，客户就会重新尝试命令给一个新领导者，导致命令重复执行。解决办法是给每个命令分配一个唯一线性数字，然后状态机追踪给每个客户最新处理的命令数字，带上相关联的回应。如果它收到已经被执行的命令数字，就立刻回应但不执行。 只读操作可以无需写入日志来处理，但没有额外措施，可能会导致返回陈旧数据。因为响应请求的领导者可能被取代，但它并不知情。线性化的读操作必须避免返回陈旧数据，Ratf采取额外两种预防措施，在不使用日志的情况下保证这一点： 确保领导者拥有最新的已提交条目。领导者完整性属性保证领导者拥有所有已提交条目，但是在任期开始时它可能不知道哪些条目已提交，为了确定这一点，领导者需要在任期内提交一个空白的条目来实现。 检查领导者是否已被取代。在处理只读请求之前，领导者必须确认自己是否仍然有效。Raft要求领导者与集群中大多数节点交换心跳信息来确保自己仍然是当前有效的领导者。 ","date":"2025-02-20","objectID":"/raft/:8:0","tags":["Distributed Systems","Consensus Algorithm"],"title":"Raft论文阅读记录","uri":"/raft/"},{"categories":["MIT6.5840"],"content":"Key/Value Server 本次试验中，将为一台机器构建一个键值服务器，该服务器确保即使网络故障，每个操作也只执行一次，且操作是线性化的。后面的实验将会复制这样的服务器来处理服务器崩溃。 线性一致性：最强的数据一致性保证，所有操作看起来像是按照某种全局顺序执行的，与真实的时间顺序一致，系统表现的像是一个单机系统，所有操作都是原子的。（分布式锁、金融系统） 顺序一致性：操作按照某种顺序执行，但不一定与真实时间一致。（多线程编程模型） 最终一致性：系统最终会达到一致状态，但中间可能存在不一致。（DNS 系统、分布式缓存） 因果一致性：保证因果关系的操作顺序，但不保证无关操作的顺序。（社交网络中的消息传递） ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:0:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"执行概述 Clients可以向KVServer发送三种不同的RPC请求：Put(key, value), Append(key, arg), Get(key)。KVServer维护键值对的内存映射map。每个Client通过一个带有Put/Append/Get方法的Clerk与服务器进行对话。Clerk管理RPC与服务器的交互。 Put(key, value)：设置或者替换一个特定键在map中的值。 Append(key, arg)：将arg附加到键对应的值后面，并返回旧值。 Get(key)：返回键对应的值，如果键不存在则返回空字符串。 提供线性一致性会使应用程序更方便，因为它是你在服务器上看到的行为，该服务器一次处理一个请求。当一个客户从服务器成功响应更新请求，则随后从其他客户端启动的读取就可以保证看到更新的效果。对于单个服务器来说，提供线性一致性也相对容易。 ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:1:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"结构体定义 // client.go type Clerk struct { server *labrpc.ClientEnd requestId int64 } // server.go type KVServer struct { mu sync.Mutex // Your definitions here. kvMap map[string]string pushAppendCache map[int64]PutAppendReply } // Put or Append type PutAppendArgs struct { Key string Value string RequestId int64 } type PutAppendReply struct { Value string ReplyId int64 } type GetArgs struct { Key string } type GetReply struct { Value string } ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:2:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"client.go ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:3:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"MakeClerk 初始化Clerk，赋值server，用nrand()创建一个requestId ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:3:1","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"Get 创建GetArgs{Key: key}和GetReply{}，用来发送RPC请求 循环发送RPC请求处理Get，直到响应成功时，返回reply.Value。 ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:3:2","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"PutAppend put和append操作的公共部分。 创建PutAppendArgs{Key: key, Value: value, RequestId: ck.requestId}和PutAppendReply{}，用来发送RPC请求。 根据传入的参数op，循环发送RPC请求处理Put/Append，直到响应成功时，返回reply.Value，且requestId自增1。 ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:3:3","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"server.go ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:4:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"Get 获取锁，避免脏读到中间状态。 读取kvMap对应key的值，若存在，则把值返回给reply，否则返回空字符串。 ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:4:1","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"Put 获取锁。 查看pushAppendCache，若存在该requesID，则把对应的值返回给reply，然后return。 否则，设置kvMap对应key的值，然后把该requestId和reply添加到pushAppendCache中。 清理上一个请求的缓存。 delete(kv.pushAppendCache, args.RequestId-1) 这里的pushAppendCache的作用是，如果客户端发送了Put/Append请求，但服务器崩溃了，那么客户端会重新发送请求，这里就可以检查requestID是否存在于缓存，保证每个操作只执行一次。 ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:4:2","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"Append 处理过程与Put相同，区别在于Put是直接修改对应key的值，而Appen的是把值附加到对应key的值后面。 ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:5:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"StartKVServer 初始化服务器的kvMap和pushAppendCache。 kv.kvMap = make(map[string]string) kv.pushAppendCache = make(map[int64]PutAppendReply) ","date":"2025-02-18","objectID":"/mit6.5840-kvserver/:6:0","tags":["Distributed Systems"],"title":"Mit6.5840 KVServer","uri":"/mit6.5840-kvserver/"},{"categories":["MIT6.5840"],"content":"MapReduce 这次实验将构建一个MapReduce系统。需要实现一个worker进程，它调用应用程序Map和Reduce函数并处理阅读和写入文件，以及一个coordinator进程，它将任务分发给worker并处理失败的worker。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:0:0","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"编程模型 计算的输入是一系列键值对，输出也是一系列键值对，可以把MapReduce的计算过程表示为两个函数：Map和Reduce。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:1:0","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"Map Map由用户实现，用一个键值对产生一系列中间键值对，然后将所有具有相同Key I的中间键值对组合在一起并发送给Reduce函数。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:1:1","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"Reduce Reduce也是由用户实现，接受一个Key I和一些具有这个Key I的值，它会合并这所有的值，产生尽可能更少数量的值。一般来说，每个Reduce过程只会产生一个或者零个输出值。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:1:2","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"Example 下面伪代码中，处理的问题是计算大量文档中每个单词出现的次数。 map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, \"1\"); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); map函数根据文件名和文件内容，每次发出单词加上一个计数，reduce函数总和某个特定单词的所有计数。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:1:3","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"执行概述 Map的调用是分布在多个机器上的，且输入数据被自动拆分为M个小块，每个拆分块可以在不同的机器上并行处理。Reduce是通过使用拆分函数（例如，hash(key) mod R）将中间键空间分割成R块来调用。拆分数量（R）和拆分函数由用户指定。 下图描述了MapReduce的执行过程： 用户程序中的MapReduce库首先将输入文件拆分为M块，通常每块16MB到64MB（用户可以通过可选参数进行控制）。然后，它在机器集群上启动程序的许多副本。 其中一个程序的副本很特别——master，其余的是master分配工作的worker。有M个map任务和R个reduce任务要分配。master选择闲置的worker，并为其分配一个map任务或reduce任务。 被分配到map任务的worker会读取相应输入拆分的内容。它从输入数据中解析键值对，并将每对传递给用户定义的Map函数。Map函数生成的中间键值对在内存中缓冲。 缓冲对会定期写入本地磁盘，由拆分函数分区为R个区域。本地磁盘上这些缓冲对的位置被传递回master，master负责将这些位置转发给reduce workers。 当master通知reduce workers这些位置时，它会使用远程过程调用RPC从map workers的本地磁盘中读取缓冲数据。当reduce workers读取了所有中间数据时，它会按中间键对数据进行排序，以便将所有出现的同一键分组在一起。需要排序是因为通常许多不同的键映射到相同的reduce任务。如果中间数据量太大，无法放入内存中，则使用外部排序。 reduce workers迭代排序的中间数据，对于每个输入的唯一中间键，它将键和相应的中间值集传递给用户的reduce函数。Reduce函数的输出被附加到此reduce分区的最终输出文件中。 当所有map任务和reduce任务都已完成时，master会唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码。成功完成时，mapreduce执行的输出应该在R个文件里（每个reduce任务生成一个），通常不需要把它们合并到一起。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:2:0","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"定义结构体 rpc.go type Task struct { TaskType int FileName string //存储map任务的单个文件 FileNames []string //存储reduce任务的多个中间文件 TaskId int ReduceNum int } coordinator.go type TaskMetaInfo struct { TaskAddr *Task state int BeginTime time.Time //记录开始时间，用来计算超时 } type TaskMetaHolder struct { MetaMap map[int]*TaskMetaInfo //存储所有任务信息 } type Coordinator struct { State int MapChan chan *Task ReduceChan chan *Task ReduceNum int Files []string taskMetaHolder TaskMetaHolder mu sync.Mutex } 在Coodinator中，用Go的channal来实现同步，保证线程安全，但仍然有部分公共资源的访问需要加锁，如taskMetaHolder。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:3:0","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"worker.go ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:4:0","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"Worker 循环执行GetTask()来获取任务，若 任务类型为MapTask，则调用DoMapTask函数，执行完后调用TaskDone函数来告诉coordinator已经执行完毕。 任务类型为ReduceTask， 则调用DoReduceTask函数，执行完后调用TaskDone函数来告诉coordinator已经执行完毕。 任务类型为WaitingTask，则调用time.Sleep(time.Second)，等待一秒。 任务类型为ExitTask，则退出循环。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:4:1","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"GetTask 模仿CallExample的流程，利用RPC来调用Coordinator中的PullTask函数，请求一个任务。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:4:2","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"DoMapTask 打开并读取FileName的文件，把文件名和内容传入mapf函数，返回一系列中间键值对。 对键用哈希，把结果分类放入二维数组中。 HashKv := make([][]KeyValue, reduceNum) for _, kv := range(intermediate) { index := ihash(kv.Key) % reduceNum HashKv[index] = append(HashKv[index], kv) } 根据实验指导中的提示，利用json.NewEncoder，把中间键值对存入文件中，键的哈希值相同的放在同一个文件中，文件命名方式为 \"mr-tmp-\" + task.TaskId + \"-\" + index，每个map任务产生reduceNum个文件。如此一来，不同index对应的中间文件里的键就不相同，分布式并行处理起来互不干扰，每个reduce worker只需获取属于自己index的那些文件来计算。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:4:3","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"DoReduceTask 根据task.FileNames，打开并读取多个中间文件，把所有键值对放在一个数组中，然后用sort排序（同理mrsequential.go）。 输出文件命名为\"mr-out-\" + task.TaskId。 遍历排序后的中间键值对，计算相同的键出现的次数，并写入输出文件中。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:4:4","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"TaskDone 用RPC调用Coordinator的MarkDone函数，通知Coordinator已经执行完毕。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:4:5","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"coordinator.go ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:0","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"MakeCoordinator 初始化Coordinator结构体。 执行c.MakeMapTasks函数，生成map任务。 执行go c.CheckTimeOut()，定期检查任务是否超时。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:1","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"(c *Coordinator) MakeMapTasks 遍历所有输入文件，每个文件生成一个map任务，并放入c.MapChan中，并维护c.taskMetaHolder信息。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:2","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"(c *Coordinator) MakeReduceTasks 生成ReduceNum个reduce任务，并放入c.ReduceChan中，并维护c.taskMetaHolder信息。 初始化task的时候，TaskId从len(c.Files)+1开始，即跟在map任务后面。 初始化task的时候需要传入FileNames，即map任务产生的中间文件，例如最开始的一个reduce任务需要文件mr-tmp-0-0，mr-tmp-1-0，mr-tmp-2-0…筛选最后一个数字为0的中间文件。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:3","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"PullTask 供worker的GetTask远程调用，来分配任务给worker。 由于只能所有map任务执行完之后才能进入到reduce阶段，所以这里先要check是否当前所有任务都执行完毕，若已经执行完毕，就进入下一个阶段，若下一阶段是reduce，就需要执行c.makeReduceTasks()，若当前已经是reduce阶段，则进入到ALLDone阶段。这一部分访问公共资源，需要加锁。 接下来判断若当前c.State为MapState，则从c.MapChan中取一个任务给*reply，然后初始化一下TaskMetaInfo信息，设置任务的state和BeginTime。 若当前c.State为ReduceState，则从c.ReduceChan中取一个任务给*reply，同样的方式初始化。 否则返回一个ExitTask类型的任务，告知worker退出。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:4","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"MarkDone 访问公共资源，上锁。把taskMetaHolder中对应任务的meta.state设置为Done。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:5","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"CheckTimeOut 执行循环，开始调用time.Sleep，每两秒检查一次是否有任务超时，访问公共资源，上锁。 如果c.State为ALLDone，则直接返回。 依次访问所有c.taskMetaHolder.MetaMap中的任务，如果任务超时，就停止该任务，并重新放入任务Channal中。 for _, v := range c.taskMetaHolder.MetaMap { if v.state == Working \u0026\u0026 time.Since(v.BeginTime) \u003e 10*time.Second { // 超时了 if v.TaskAddr.TaskType == MapTask { v.state = Waiting c.MapChan \u003c- v.TaskAddr } else if v.TaskAddr.TaskType == ReduceTask { v.state = Waiting c.ReduceChan \u003c- v.TaskAddr } } } ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:6","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["MIT6.5840"],"content":"Done 这个函数是给主函数中调用，来检查是否所有任务都执行完毕。这里只需查看c.State是否为ALLDone，若为ALLDone，则返回true，否则返回false。 ","date":"2025-02-15","objectID":"/mit6.5840-mapreduce/:5:7","tags":["Distributed Systems"],"title":"Mit6.5840 MapReduce","uri":"/mit6.5840-mapreduce/"},{"categories":["CMU"],"content":"Concurrency Control 在这个项目中，你将通过实现乐观多版本并发控制（MVOCC）来为BusTub添加事务支持。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:0:0","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Timestamps ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:1:0","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Timestamp Allocation 当一个事务开始时，它将被分配一个读时间戳，这是最近提交的事务的提交时间戳。在事务提交时，它将被分配一个单调递增的提交时间戳。读时间戳确定事务可以读取哪些数据，而提交时间戳确定事务的序列化顺序。 比如这个例子： 提交时间戳是一个逻辑计数器，每次提交事务时递增1。D3元组由提交时间戳为3的事务写入。 如果我们有一个读时间戳 = 3的事务，在这个例子中它就可以看到A3，B3，C2和D3。如果读取时间戳= 2，则会看到A2，B1，C2。当事务开始时，读取时间戳将是最近提交的事务的时间戳，因此事务将能够看到在事务开始之前提交的所有内容。你无法读到未来的数据。 在TransactionManager::Begin中设置一个事务的read_ts_为last_commit_ts_，使用c++原子变量atomic。 txn_ref-\u003eread_ts_.store(last_commit_ts_); 在TransactionManager::Commit中设置commit_ts_为last_commit_ts_+1，也使用c++原子变量atomic。然后last_commit_ts_自增1。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:1:1","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Watermark 水印是所有正在进行的事务中最低的读时间戳。实现用尽量少的时间复杂度。 定义一个map来存储每个读时间戳有多少个。 Watermark::AddTxn中把传入的读时间戳在map中加1，map里没有就设为1。watermark_设置为map.begin()-\u003efirst。 Watermark::RemoveTxn中把传入的读时间戳在map中减1，如果减到0了就erase掉。如果map为不空就设置为map.begin()-\u003efirst，为空就设置为commit_ts_。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:1:2","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Storage Format and Sequential Scan BusTub将事务数据存储在三个地方：表堆、事务管理器和每个事务内部。表堆总是包含最新的数据，事务管理器“页面版本信息”存储指向下一次修改的指针，在每个事务中，我们以称为undo log的格式存储事务修改的元组。要在给定的读取时间戳检索元组，你需要 获取所有修改（undo logs） 将修改（“undo” the undo logs）应用于元组的最新版本以恢复元组的过去版本。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:2:0","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Tuple Reconstruction 在本任务中，你将需要实现元组重构算法。你需要实现ReconstructTuple函数。ReconstructTuple接受存储在表堆中的基本元组和基本元数据，以及按它们添加到系统的时间降序排列的撤销日志列表。ReconstructTuple将始终应用提供给函数的所有修改，而无需查看元数据或撤消日志中的时间戳。 下面介绍一个重建元组的例子： base tuples总是包含完整的数据，但是在undo logs中只包含修改过的列。modified_fields是一个bool向量，其长度与表模式相同，设置为true的地方表示改字段被修改。日志中的的元组字段只包含修改过的字段。同时你需要考虑is_delete标志来处理元组被删除的情况。 判断若最后一个日志is_deleted_为true，说明数据最后被删了，直接返回std::nullopt。或者如果日志为空，base tuple显示被删除，也返回std::nullopt。 遍历undo logs：对每个撤销日志，检查modified_fields_，把所有true的地方，依次记录到一个新的vectorcols后面。根据cols，用CopySchema方法构造修改元组的schema。 按照schema的列顺序重构元组，对每个列，判断是否在cols中，如果在就用日志中partial tuple的值，否则就用base tuple的值。最终得到重建一个日志后的元组，直到遍历完所有日志。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:2:1","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Sequential Scan / Tuple Retrieval 在这个任务中，你需要重写Project #3中的顺序扫描执行器，以便支持基于事务的读取时间戳从过去检索数据。 顺序扫描执行器扫描表堆，检索直到事务读取时间戳的撤销日志，然后重建将用作执行器输出的原始元组。在MVCC顺序扫描执行器中需要处理3种情况： 表堆中的元组是最新的数据。可以直接返回元组，如果元组已经被删除则跳过元组。 表堆中的元组包含当前事务的修改。在BusTub中，正常的有效ts范围是0到TXN_START_ID - 1。若是表堆中的元组被还未提交的事务修改，那么该元组的ts会设置为TXN_START_ID + txn_human_readable_id，将是一个非常大的数字。所以如果发现元组就是被当前事务修改的，应该直接返回给用户，否则恢复过去版本。 表堆中的元组被另一个未提交的事务修改，或者比事务读时间戳新。在这种情况下，你将需要重新配置版本链，以收集读取时间戳之后的所有撤消日志，并恢复元组的过去版本。 举个例子，下面的示例中的TXN_START_ID将为1000。因此，1009指示元组包含txn9的未提交更新。 Txn9尚未提交，读取时间戳为3。在表格的txn 9中顺序扫描的结果将是：（A，9），（B，9），（C，2），（D，9）。 考虑另一个读时间戳为4的事务，该事务的顺序扫描的结果将是：（A，3），（B，3），（C，4）。 在原先seqscan基础上，判断如果是当前事务修改meta.ts_ == txn_-\u003eGetTransactionId()或者小于事务读取时间戳meta.ts_ \u003c= txn_read_ts_，即可直接返回。 否则这个元组的数据要么是来自未来，要么是正在被另一个事务修改，我们需要重建该元组：获取第一个日志，若没有日志或者无效日志，则跳过，否则收集日志，知道时间戳小于等于事务读时间戳，然后重建元组。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:2:2","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"MVCC Executors 在本节中，你将需要实现数据修改执行器，包括插入执行器、删除执行器和更新执行器。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:3:0","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Insert Executor 在这个项目中，你的insert executor实现应该与项目#3中的几乎相同。你可以简单地在表堆中创建一个新的元组。你需要正确地设置元组的元组元数据。表堆中的时间戳应该设置为事务临时时间戳。你还应该将RID添加到写入集。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:3:1","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Commit 一次只允许一个事务进入Commit函数，你应该通过在事务管理器中使用commit_mutex_来确保这一点。在这个任务中，需要在事务管理器中用提交逻辑扩展你的Commit实现。 获取commit锁，遍历写入集合，将base tuples的时间戳设置为提交时间戳。 设置事务的提交状态并更新事务的提交时间戳和last_committed_ts ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:3:2","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Update and Delete Executor 在这个任务中，你需要实现生成撤消日志和更新表堆元组的逻辑。其中更新执行器将元组的新版本放入表堆中，删除执行器为表堆中的元组设置is_delete标志。 在更新或删除元组之前，你需要检查写-写冲突。 如果未提交的事务正在修改元组，则不允许其他事务修改它，如果它们这样做，则将出现写-写冲突，并且与前一个事务冲突的事务应该被中止。 另一种写-写冲突情况是当事务A删除元组并提交，而另一个事务B在A之前开始并删除相同的元组（写入比读ts大的元组）。 更新执行器应该实现为pipeline breakers：在写入任何更新之前，它应该首先扫描子执行器到本地缓冲区的所有元组。之后，它应该从本地缓冲区中提取元组，计算更新的元组，然后在表堆上执行更新。 看这个例子： 在情况（1）中，txn 10删除（A，2）元组并且还没有提交。Txn 9仍然可以读取元组（A，2）的旧版本，因为读取时间戳是3。在这种情况下，如果Txn 9需要更新/删除元组，则应该通过写-写冲突中止。 在情况（2）中，如果任何其他事务尝试更新/删除该元组，则它们将被中止。 在情况（3）中，存在将（C，2）更新为（C，4）的另一个事务，并且提交时间戳被设置为4。Txn 9可以读取元组（C，2）的旧版本，但是当更新/删除（C，4）元组时，它应该被中止，因为在事务读取时间戳之后发生了更新。 实现中还需要考虑自我修改，这应该在检查写-写冲突之前完成。如果一个元组已经被当前事务修改过，它可以自己更新/删除，你不应该把这看作是写-写冲突。如果元组是新插入的，则不需要创建撤消日志。否则，事务对于每个RID最多保留一个撤消日志。因此，如果一个事务更新一个元组两次，你需要更新之前的撤销日志/只更新表堆。 delete 先检查写-写冲突，如果meta上记录的时间戳大于当前事务的读时间戳，并且不等于事务的临时时间戳，则冲突。 如果是自我修改，判断是否有第一个日志，如果有，且日志和meta都未标记删除，重建一次元组，然后直接用它修改第一个日志中的元组值，并设置为标记为删除。 如果不是自我修改，生成日志，日志中包含所有字段，并设置为标记为删除。类似链表的插入操作，插入到base tuple和第一个日志之间。 update 与删除类似。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:3:3","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Stop-the-world Garbage Collection 现在的代码中，一旦事务添加到事务map中，我们就永远不会删除它，因为具有较低读取时间戳的事务可能需要读取存储在先前提交或中止的事务中的撤销日志。在本任务中，你需要实现一个简单的垃圾收集策略，以删除未使用的事务。 当调用GarbageCollection时，会手动触发垃圾收集。在任务1中，我们已经实现了一个算法来计算水印（系统中最低的read_ts）。在此任务中，你将需要删除不包含任何对具有最低read_ts（水印）的事务可见的撤消日志的所有事务。 上面的示例说明了水印时间戳为3并且我们提交了txn1，txn2，txn9的情况。Txn1的撤销日志不再可访问，所以我们可以直接删除它。Txn2的元组（A，2）的撤销日志不可访问，但元组（C，2）仍然可访问（因为它是第一个日志，ts为3的事务会访问到它），所以我们现在不能删除它。 遍历事务map中的所有事务，再遍历每个事务中的写入集里的每个元组。 对每个元组，遍历日志，跳过第一个小于等于水印的日志，对之后的日志，is_deleted设置为true。 上述操作结束后，遍历事务map，对COMMITTED的事务，查看是该事务所有的UndoLog都标记为删除，如果是，则记录下来，遍历完后erase掉这些事务。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:3:4","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Primary Key Index 当创建表时指定主键时，BusTub将自动创建一个索引，其is_primary_key属性设置为true。一个表最多只能有一个主键索引。主键索引确保主键的唯一性。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:4:0","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Index Scan, Deletes and Updates 在此任务中，你需要为删除和更新执行器添加索引支持。首先实现多版本索引扫描执行器，然后实现对插入、更新和删除执行器的更新和删除支持。 Index Scan 在哈希表中ScanKey寻找，若返回为空则return false。 不为空则判断是否是自我修改的元组或者时间戳小于当前事务的读时间戳，然后判断元组若是已删除则return false，否则获取元组。 若不是第二条的情况，说明要遍历日志，将base tuple重构直到一个自己事务可读的日志。其中要判断日志是否存在且有效。 Deletes and Updates 一旦在索引中创建了一个条目，它将始终指向同一个RID，即使元组被标记为已删除，它也不会被删除，因此早期的事务仍然可以使用索引扫描执行器访问历史。此时，你还需要重新访问插入执行器。考虑插入执行器插入到一个元组中的情况，该元组被删除执行器删除。你的实现应该更新已删除的元组，而不是创建新的条目，因为索引条目一旦创建就始终指向同一个RID。你需要正确处理写-写冲突检测和唯一约束检测。 此时，你还需要考虑其他的竞争条件，若是多个事务同时更新版本链接，则只能允许其中一个继续进行，终止其他的事务。在日志信息中，有一个in_progress字段，表示这个元组是否已经有一个正在进行的事务。 我理解的是，多个进程竞争时，对in_progress的修改也不是原子操作，可能有几个进程一起获得in_progress，一些进程没有被正确的终止。所以事务在操作版本信息的时候，考虑对这个元组做类似上锁的操作，保证原子性的情况下获取in_progress字段，而阻止剩下的进程。 auto LockRID(RID rid, TransactionManager *txn_mgr) -\u003e bool { auto version_link = txn_mgr-\u003eGetVersionLink(rid); if (version_link.has_value()) { return (txn_mgr-\u003eUpdateVersionLink(rid, VersionUndoLink{UndoLink{version_link-\u003eprev_}, true}, [version_link](std::optional\u003cVersionUndoLink\u003e origin_version_link) -\u003e bool { return origin_version_link.has_value() \u0026\u0026 !origin_version_link-\u003ein_progress_ \u0026\u0026 origin_version_link-\u003eprev_ == version_link-\u003eprev_; })); } return txn_mgr-\u003eUpdateVersionLink( rid, VersionUndoLink{UndoLink{}, true}, [](std::optional\u003cVersionUndoLink\u003e version_link) -\u003e bool { return !version_link.has_value(); }); } 所以创建一个LockRID的函数，注意到执行UpdateVersionLink会传入一个check函数，我们定义一个check函数来保证，在获取in_progress_的过程中，没有其他进程进入，否则获取失败。 然后需要在之前的执行器中，涉及日志版本信息修改的时候，都需要先执行LockRID，然后才能进行修改。 but…测试中InsertConcurrency和UpdateConcurrency有时能过有时过不了，还没找到原因。 ","date":"2025-01-02","objectID":"/cmu15445_concurrency_control/:4:1","tags":["database"],"title":"Cmu15445 Concurrency Control","uri":"/cmu15445_concurrency_control/"},{"categories":["CMU"],"content":"Query Execution 在这次项目中，将实现允许 BusTub 执行查询的组件。具体是创建执行 SQL 查询的运算符执行程序，并实施优化器规则来转换查询计划。 BusTub的架构如下： BusTub支持EXPLAIN命令打印查询的执行计划，比如： EXPLAIN的结果中展示了查询处理层的转换过程。该语句首先有Parser和Binder处理，后者生成表示查询的抽象语法树（AST）。在这个例子中，查询由 __mock_table_1 上的 BoundSelect 表示，该 BoundSelect 将检索两列（colA 和 colB）。注意Binder 会自动将原始 SQL 查询中的 * 字符扩展到表中的实际列。 接下来，Binder AST由Planner处理，将生成适当的查询计划。这个例子中，查询计划是一个包含两个节点的树，数据从叶子流向根。 之后，优化器（Optimizer）将会优化查询计划，他会删除冗余的投影。 接下里考虑一个更复杂的例子： 这里经过优化器优化后的查询计划为： ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:0:0","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"注意 在开始之前，先对System Catalog有一个了解。数据库维护一个内部目录来跟踪关于数据库的元数据。在本项目中，你将与系统编目交互，以查询有关表、索引及其架构的信息。 对于表修改执行器（InsertExecutor、UpdateExecutor和DeleteExecutor），必须修改操作所针对的表的所有索引。你会发现Catalog：：GetTableIndexes（）函数可以查询为特定表定义的所有索引。一旦为每个表的索引创建了IndexInfo实例，就可以对基础索引结构调用索引修改操作。在这个项目中，将使用你在项目#2中实现的哈希表索引作为所有索引操作的底层数据结构。 BusTub优化器是一个基于规则的优化器。大多数优化器规则以自底向上的方式构造优化计划。由于查询计划具有此树结构，因此在将优化器规则应用于当前计划节点之前，需要首先将规则递归地应用于其子节点。在每个计划节点，你应确定源计划结构是否与你尝试优化的计划结构匹配，然后检查该计划中的属性，以查看是否可以将其优化为目标优化计划结构。（仓库中已经提供了几个优化器规则的实现，提前看一下作为参考。） ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:1:0","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Access Method Executors 此次任务，将要实现对存储系统中的表进行读取和写入的执行程序, 下面将依次介绍每一个执行程序。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:0","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"SeqScan SeqScanPlanNode 可以使用 SELECT * FROM table 语句进行规划。 SeqScanExecutor 迭代一个表，并一次返回一个 Tuples。 Hint: 不要输出在 TableHeap 中删除的元组。检查每个元组对应 TupleMeta 的 is_deleted_ 字段。 顺序扫描的输出是每个匹配的元组及其原始记录标识符 （RID） 的副本。 构造函数中能赋值的赋值。**Init()**中获取table_heap（table的oid在plan_中），用table_heap的MakeIterator()创建迭代器，用迭代器遍历，记录下所有的RID。 Next(Tuple *tuple, RID *rid)。遍历记录好的RID们，对应的tuple本身和tuplemeta都可以通过table_heap获取，若meta中显示没被删除，则赋值给参数中的tuple和rid作为返回值。 循环的判断条件注意要加上filter评估的结果转换为bool类型，若是最后一个rid也过去了就return false，否则return true。 do { ... } while (meta.is_deleted_ || (plan_-\u003efilter_predicate_ != nullptr \u0026\u0026 !plan_-\u003efilter_predicate_ -\u003eEvaluate(tuple, GetExecutorContext()-\u003eGetCatalog()-\u003eGetTable(plan_-\u003eGetTableOid())-\u003eschema_) .GetAs\u003cbool\u003e())); return true; ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:1","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Insert 可以使用 INSERT 语句来规划 InsertPlanNode。请注意，你需要使用单引号来指定 VARCHAR值。 InsertExecutor将Tuples插入表中并更新受影响的索引。它只有一个子项就是生成要插入到表中的值。Planner确保这些值与表有相同的架构。Executor 将生成一个整数类型的 Tuples 作为输出，指示已插入到表中的行数。如果有与之关联的索引，请记住在插入表时更新索引。 Init()，先对child_executor_执行初始化，然后将has_isnerted_设置为false，来避免halloween problem。（包括后面的执行器也都需要这一步） Next(Tuple *tuple, RID *rid)，先判断has_inserted如果为true，说明已经插入过了，直接返回false。否则置为true然后开始插入。获取需要用到的table_info、schema、indexes，并初始化一个count来记录插入行数。 从子执行器child_executor_ 中逐个取出元组并插入到表中，同时更新所有的索引。利用InsertTuple返回得到的new_rid，遍历indexes中的每个索引类型，先用tuple-\u003eKeyFromTuple拿到key，然后用index_-\u003eInsertEntry更新索引。 根据要求，用count构造一个tuple返回。 std::vector\u003cValue\u003e result = {{TypeId::INTEGER, count}}; *tuple = Tuple(result, \u0026GetOutputSchema()); return true; ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:2","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Update UpdatePlanNode 可以使用 UPDATE 语句进行规划。它只有一个子项，其中包含表中要更新的记录。 UpdateExecutor修改指定表中的现有元组。执行器将生成一个整数类型的元组作为输出，指示有多少行被更新。请记住更新受更新影响的任何索引。要实现更新，首先删除受影响的元组，然后插入一个新的元组。 前两步与insert相同。每次从子执行器得到的元组，先删除再插入。 删除的操作是UpdateTupleMeta(TupleMeta{0, true}, *rid)把meta更新一下就可以了，然后还要删除对应的索引，用DeleteEntry。 再插入之前，先要计算表达式，得到更新后的值，然后再执行插入，与insert相同。 // update: compute expressions std::vector\u003cValue\u003e new_values{}; for (auto \u0026expr : plan_-\u003etarget_expressions_) { new_values.push_back(expr-\u003eEvaluate(tuple, child_executor_-\u003eGetOutputSchema())); } ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:3","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Delete DeletePlanNode可以用一个SQL语句来规划。它正好有一个子节点，该子节点具有要从表中删除的记录。 DeleteExcutor应该生成一个整数输出，表示从表中删除的行数。它还需要更新任何受影响的索引。要删除一个元组，你需要从子执行器获取一个RID，并更新该元组对应的TupleMeta的is_deleted_field。 同理update的删除部分的逻辑。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:4","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"IndexScan IndexScanExecutor使用散列索引进行点查找，以检索元组的RID。然后操作员使用这些RID在相应的表中检索它们的元组。然后它每次发出一个元组。 计划中索引对象的类型在此项目中将始终为 HashTableIndexForTwoIntegerColumn 。你可以安全地将其转换并存储在executor对象中： htable_ = dynamic_cast\u003cHashTableIndexForTwoIntegerColumn *\u003e(index_info_-\u003eindex_.get()) 你可以使用散列索引进行点查找，从表堆中查找元组，并根据谓词输出正确的元组。BusTub仅支持具有单个唯一整数列的索引。我们的测试用例不会包含重复的键。因此，点查找只返回一个元组，如果它存在。不要发出已删除的元组。 **Init()**的时候，用上面说的，获取htable_存储在executor中。 Next()：利用htable_的ScanKey方法搜索，若返回结果为空，说明没有这个索引，直接返回false，否则从table_heap中拿到这个元组查看meta中的is_deleted_，若没被删除就正常返回。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:5","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Optimizing SeqScan to IndexScan 当我们查询索引列时，使用IndexScan将显著提高查找性能。为此，我们需要将过滤器下推到扫描器中，以便我们知道在索引中查找的键。然后我们可以直接通过索引检索值，而不是进行全表扫描或索引扫描。 这里需要修改优化器，以便在可能的情况下将SeqScanPlanNode转换为IndexScanPlanNode。如执行bustub\u003e EXPLAIN (o) SELECT * FROM t1 WHERE v1 = 1;，不应用MergeFilterScan和SeqScan as IndexScan优化器规则，计划可能如下所示： 在应用MergeFilterScan和SeqScan as IndexScan优化器规则之后，我们可以只做一个快速的索引查找，而不是迭代整个表。生成的计划如下所示： 执行之前，递归的优化子节点。 for (const auto \u0026child : plan-\u003eGetChildren()) { optimized_children.emplace_back(OptimizeSeqScanAsIndexScan(child)); } auto optimized_plan = plan-\u003eCloneWithChildren(std::move(optimized_children)); 如果optimized_plan-\u003eGetType()等于PlanType::SeqScan，开始执行优化。获取计划的谓词，如果谓词为空仍然执行顺序扫描。 判断其是否为条件谓词中的等值条件ComparisonType::Equal，只有等值条件这里才转化为索引扫描。 根据谓词的列，获取索引信息col_index。遍历indexes，如果index_-\u003eGetKeyAttrs()取出的列与col_index相同，表示存在相关索引，构建IndexScanPlanNode并用智能共享指针来管理，返回。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:2:6","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Aggregation \u0026 Join Executors ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:3:0","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Aggregation AggregationPlanNode用于支持以下查询: 聚合执行器为每组输入计算聚合函数。它只有一个孩子。输出架构由group-by列和聚合列组成。实现聚合的一个常见策略是使用哈希表，并将group-by列作为键。 在本项目中，你可以假设聚合哈希表适合内存。这意味着你不需要实现基于分区的多级策略，并且哈希表不需要由缓冲池页支持。我们提供了一个SimpleAggregationHashTable数据结构，它公开了一个内存中的哈希表（std：：unordered_map），但带有一个为计算聚合而设计的接口。这个类还公开了一个 Iterator类型，可用于遍历哈希表。你需要完成此类的CombineAggregateValues函数。 Hint中提到，聚合是pipeline breakers，意味着在流水线中，聚合会中断数据流处理的过程。你会希望Init()中尽可能多地做准备工作，因为聚合往往需要看到所有相关的输入行才能正确地生成最终的结果。 在实现aggregation之前需要先把哈希表完善一下。主要是CombineAggregateValues，将输入的聚合值与结果聚合值进行合并：遍历所有聚合表达式。根据不同的聚合类型（如计数、求和、最小值、最大值等），对每个聚合值进行相应的操作。 Init()：子执行器初始化，根据plan_的信息创建SimpleAggregationHashTable。遍历子执行器，将子执行器中的获取的数据插入到聚合哈希表中，不能在聚合执行器中完成，因为聚合执行器需要先从子执行器中获取所有数据，然后对这些数据进行分组和聚合操作，最后才能产生输出结果。具体来说，对每一个元组，执行MakeAggregateKey和MakeAggregateValue获取key和value，再InsertCombine到AHT里。 Next()：利用Iterator遍历AHT，如果有groupby的话就创建一个vector\u003cValue\u003e，先把groupby都依次添加进去，然后把聚合值依次添加进去，用这个vector构造tuple返回。groupbys为空，则生成一个初始的聚合值元组返回。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:3:1","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"NestedLoopJoin 默认情况下，DBMS将对所有连接操作使用NestedLoopJoinPlanNode。考虑以下示例查询： 你需要使用类中的简单嵌套循环连接算法为NestedLoopJoinExecutor实现内部连接和左连接。该操作符的输出模式是左表中的所有列，后面跟着右表中的所有列。对于外部表中的每个元组，考虑内部表中的每个元组，如果满足连接谓词，则发出输出元组。 左连接：返回左表中的所有记录，如果右表中没有匹配的记录，就用NULL表示。 内部连接：仅返回两个表中有匹配的结果行。 Init()：初始化左右两个子执行器。 Next()：（这里解释左连接，内部连接类似）对于每个左表中的元组，遍历右表，用谓词的EvaluateJoin来判断是否匹配，若匹配成功，创建一个values，把左元组每一列的值添加进去，再把右元组每一列的值添加进去，用这个values构造一个tuple，返回。 若是遍历完右表没找到，则用NULL代替右元组的值。 ValueFactory::GetNullValueByType(right_executor_-\u003eGetOutputSchema().GetColumn(i).GetType()); ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:3:2","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"HashJoin Executor and Optimization 在开始此任务之前，你需要在任务#2中实现NestedLoopJoinExecutor。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:4:0","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"HashJoin 如果一个查询包含一个两列之间有多个等价条件的连接（等价条件由AND分隔），DBMS可以使用HashJoinPlanNode。考虑以下示例查询： 你需要使用类中的散列连接算法为HashJoinExecutor实现内部连接和左连接。与聚合一样，哈希连接的构建端是pipeline breaker。你需要一种方法来散列一个具有多个属性的元组，以构造一个唯一的键，参考SimpleAggregationHashTable。 Init()：初始化左右两个子执行器，遍历右表执行器，把每个元组插入到自己的哈希表里。 Next()：遍历左表执行器，对每个元组，通过左表的key，在哈希表GetValue到所有右表中匹配的元组，然后用和NestedLoopJoin中相同的操作构造tuple并返回。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:4:1","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Optimizing NestedLoopJoin to HashJoin 哈希连接通常比嵌套循环连接产生更好的性能。当连接谓词是两列之间的几个等价条件的合取时，可以使用哈希连接算法。对于这个项目，你应该能够处理不同数量的由AND连接的等价条件。 考虑这个例子：bustub\u003e EXPLAIN (o) SELECT * FROM test_1 t1, test_2 t2 WHERE t1.colA = t2.colA AND t1.colB = t2.colC; 如果不应用NLJAsHashJoin优化器规则，计划可能如下所示： 应用NLJAsHashJoin优化器规则后，将从NestedLoopJoinPlanNode中的单个连接谓词中提取左连接键表达式和右连接键表达式。生成的计划如下所示： 先写一个单独的函数用来解析and表达式，并提取出左右连接键表达式：如果逻辑表达式是and则递归解析两边的表达式。接下来判断是否是=表达式，用GetTupleIdx()区分哪个列属于哪个表后，将其添加到对应的键表达式中。 开始实现OptimizeNLJAsHashJoin，与之前优化器一样先对子执行器进行优化。然后检查当前plan_的类型是不是PlanType::NestedLoopJoin，是的话开始优化：获取谓词，用刚才的函数提取左右两侧键表达式，用得到的结果创建一个新的HashJoinPlanNode返回。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:4:2","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Sort + Limit Executors + Window Functions + Top-N Optimization 在开始此任务之前，您需要在任务#1中实现IndexScanExecutor。如果表上有索引，查询处理层将自动选择它进行排序。在其他情况下，您需要一个特殊的排序执行器来完成此操作。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:5:0","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Sort 除了ORDERBY属性与索引键匹配的情况外，BusTub将对所有ORDERBY运算符使用SortPlanNode。 此计划节点不会更改schema（即输出与输入schema相同）。您可以从order_bys中提取排序键，然后使用std::sort和自定义比较器对子对象中的所有元组进行排序。您可以假设表中的所有条目都可以容纳在内存中。 Init()：初始化子执行器，并创建一个Vector来子执行器输出的元组，使用std::sort排序。 std::sort(tuples_.begin(), tuples_.end(), Comparator(\u0026GetOutputSchema(), plan_-\u003eGetOrderBy())); Next()：依次输出排序后的元组。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:5:1","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Limit LimitExecutor约束其子执行器的输出元组的数量。如果其子执行器生成的元组数量小于计划节点中指定的限制，则此执行器无效，并生成它接收的所有元组。 Next()：每次输出的时候计数，到达plan_-\u003eGetLimit()后返回false。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:5:2","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Top-N Optimization Rule 修改BusTub的优化器以支持转换前N个查询。考虑以下查询： EXPLAIN SELECT * FROM __mock_table_1 ORDER BY colA LIMIT 10; 默认情况下，BusTub将通过以下方式执行此查询：（1）对表中的所有数据进行排序（2）获取前10个元素。这显然是低效的，因为查询只需要最小的值。一个更聪明的方法是动态地跟踪到目前为止最小的10个元素。这就是BusTub的TopNExecutor所做的。想想什么数据结构可以用来跟踪前n个元素，小顶堆和大顶堆！。 所以需要修改优化器，以支持将带有ORDER BY + LIMIT子句的查询转换为使用TopNEutor。 Init()：用优先队列创建一个堆。遍历子执行器的元组，插入堆，如果当前堆的大小超过N，则弹出一个，最后保留的即为前N个，O(n*logN) // 使用优先队列存储topN，升序用大顶堆（每次pop出来大的），降序用小顶堆 // 元素为Tuple，底层用vector存储，比较器为Comparator std::priority_queue\u003cTuple, std::vector\u003cTuple\u003e, Comparator\u003e heap(Comparator(\u0026GetOutputSchema(), plan_-\u003eGetOrderBy())); 对于优化器OptimizeSortLimitAsTopN，先判断如果plan_的类型是Limit，将其转换为LimitPlanNode来获取limit_的值，并获取plan_的第0个孩子，如果孩子的类型是Sort，就创建一个TopNPlanNode返回。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:5:3","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Window Functions 一般来说，窗口函数有三个部分：partition by、order by和window frames。这三个功能都是可选的，因此这些功能的多种组合使窗口功能一开始令人生畏。然而，窗口函数的概念模型有助于使其更容易理解。概念模型如下：* 根据partition by子句中的条件分割数据。* 然后，在每个分区中，按order by子句排序。* 然后，在每个分区（现在已排序）中，遍历每个元组。对于每个元组，我们计算该元组的框架的边界条件。每个帧都有一个开始和结束（由窗口帧的子句指定）。窗口函数是在每个帧中的元组上计算的，我们输出在每个帧中计算的结果。 ","date":"2024-12-20","objectID":"/cmu15445-query_execution/:5:4","tags":["database"],"title":"Cmu15445 Query Execution","uri":"/cmu15445-query_execution/"},{"categories":["CMU"],"content":"Buffer Pool 这个project是要在存储管理器中实现一个buffer pool，即缓冲池。缓冲池其实就是一块大的内存区域主内存，负责与磁盘之间来回移动物理页。它使得DBMS支持大于系统可用内存的数据库。缓冲池的操作对系统中的其他部分应该是透明的。例如，系统使用唯一标识符page_id_t想缓冲池请求页面的时候，系统不知道该页面是否位于已经在内存中，或者是需要从磁盘检索。 实现的时候需要保证线程安全。多个线程可以同时访问内部数据结构，并且必须确保关键部分收到latch的保护。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:0:0","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"LRU-K 更换策略 这个组件负责跟踪缓冲池的页面使用情况。LRUKReplacer的最大大小与缓冲池的大小相同，但并非 replacer 中的所有帧都被视为可驱逐。LRUKReplacer的大小由可驱逐帧的数量表示。 LRU-K 算法移出其后向 k 距离为replacer中所有帧的最大值的帧。向后 k 距离的计算方法是当前时间戳与第 k 次访问的时间戳之间的时间差。历史访问次数少于 k 次的帧将为其后 k 距离指定 +inf。 当多个帧具有 +inf 向后 k 距离时，实施普通的LRU。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:1:0","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Evict(frame_id_t* frame_id) -\u003e bool 驱逐具有最大向后 k 距离的帧。将帧 ID 存储在 output 参数中并返回 True。如果没有可驱逐的帧，则返回 False。 获取latch，判断如果当前大小为0，则返回false。 构建tuple(id, kthTimestamp, mostRecentTimestamp)来存储节点信息，然后创建一个vector\u003ctuple\u003e，遍历node_store_中的所有节点，把每个节点的信息用tuple的形式放进vector中。vector为空则返回false。 实现一个临时的cmp排序函数，对vector中元素进行排序std::sort(vec.begin(), vec.end(), cmp)，排在第一个的元素就是需要被驱逐的节点。清除节点历史、置为不可驱逐、replacer大小减1、赋值frame_id。 auto cmp = [](const std::tuple\u003cframe_id_t, size_t, size_t\u003e \u0026a, const std::tuple\u003cframe_id_t, size_t, size_t\u003e \u0026b) { if (std::get\u003c1\u003e(a) != std::get\u003c1\u003e(b)) { return std::get\u003c1\u003e(a) \u003e std::get\u003c1\u003e(b); } return std::get\u003c2\u003e(a) \u003c std::get\u003c2\u003e(b); }; ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:1:1","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"RecordAccess(frame_id_t frame_id) 记录在当前时间戳访问给定的帧 ID。应在 BufferPoolManager 中固定页面后调用此方法。 获取latch，frame_id不超过replacer_size_，否则throw exception。 当前时间戳加1，先在node_store_找frame_id，找到了就直接把当前时间戳添加到该帧的访问历史中，否则新建一个节点，添加访问历史，并把新节点emplace到node_store_中。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:1:2","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Remove(frame_id_t frame_id) 清除与该帧关联的所有访问历史记录。仅当在 BufferPoolManager 中删除页面时，才应调用此方法。 获取latch，frame_id不超过replacer_size_，否则throw exception。 在node_store_找该帧，若没找到或者找到了但不可驱逐，直接return。 拿到该帧，清除访问历史，置为不可驱逐，replacer当前大小减1。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:1:3","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"SetEvictable(frame_id_t frame_id, bool set_evictable) 该方法控制帧是否可驱逐。它还控制 LRUKReplacer 的大小。在实现 BufferPoolManager 时当页面的 pin count 达到 0 时，其对应的帧被标记为 evictable 并且 replacer 的大小会增加。 获取latch，frame_id不超过replacer_size_，否则throw exception。 在node_store_找该帧，若没找到，直接return。 找到该帧之后，判断若是由unevictable变为evictable，则replacer当前大小加1，反之replacer当前大小减1。设置该帧可驱逐与否。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:1:4","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Disk Scheduler 该组件负责调度DiskManager上的读写操作。BufferPoolManager 可以使用磁盘调度程序disk scheduler对磁盘请求进行排队，磁盘调度程序将维护一个后台工作线程，负责处理调度的请求。 磁盘调度程序将利用共享队列来调度和处理磁盘请求。一个线程将向队列添加一个请求，磁盘调度程序的后台工作人员将处理排队的请求。项目已提供了一个Channel类 src/include/common/channel.h 以促进线程之间安全共享数据。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:2:0","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Schedule(DiskRequest r) 安排DiskManager执行的请求。 DiskRequest结构指定请求是否为读/写、数据应写入/从何处以及操作的页 ID。 DiskRequest还包含一个std::promise一旦处理请求，其值应设置为 true。 将请求添加到共享队列中 request_queue_.Put(std::make_optional\u003cDiskRequest\u003e(std::move(r))); ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:2:1","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"StartWorkerThread() 此方法负责获取排队的请求并将它们分派到DiskManager。请记住在DiskRequest的回调中设置值，以向请求发出者发出请求已完成的信号。在调用 DiskScheduler的析构函数之前，该值不应返回。 循环查看是否有请求，有请求就调用DiskManager的ReadPage或WritePage方法，然后执行回调设置值。 void DiskScheduler::StartWorkerThread() { std::optional\u003cDiskRequest\u003e request; // 是否循环看request.has_value() while ((request = request_queue_.Get(), request.has_value())) { if (request-\u003eis_write_) { disk_manager_-\u003eWritePage(request-\u003epage_id_, request-\u003edata_); } else { disk_manager_-\u003eReadPage(request-\u003epage_id_, request-\u003edata_); } request-\u003ecallback_.set_value(true); } } ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:2:2","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Disk Manager 磁盘管理器类Disk Manager从磁盘读取页面数据并将其写入磁盘。您的磁盘调度程序在处理读取或写入请求时将使用磁盘管理器的ReadPage()和WritePage()。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:3:0","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Buffer Pool Manager BufferPoolManager负责使用DiskScheduler从磁盘获取数据库页面并将其存储在内存中。当明确指示执行此操作或需要逐出页面以为新页面腾出空间时， BufferPoolManager还可以安排将脏页面写入磁盘。 实验文档中强调，这里需要理解的是：系统里所有内存的页面都是由Page对象表示，BufferPoolManager也不需要了解页面的内容，但是作为开发人员，你需要认识到Page对象只是bufferpool中内存的容器。也就是说，每个Page对象都包含了一块内存空间，用来存放从磁盘读取的物理页面的内容。当数据在磁盘上来回移动时， BufferPoolManager将重用相同的Page对象来存储数据。这意味着在系统的整个生命周期中，同一个Page对象可能包含不同的物理页。 Page对象的标识符(page_id)跟踪它包含的物理页。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:0","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"NewPage(page_id_t* page_id) 当您想要在NewPage()中创建新页面时， AllocatePage私有方法为BufferPoolManager提供唯一的新页面 id。 获取latch，若free_list_不空，则取出free_list_的第一个元素（也可以不是第一个）作为frame_id，然后pop掉。否则，用LRUKReplacer的Evict方法获取一个frame_id，若驱逐失败，直接返回nullptr，驱逐成功则先从page_table_中erase掉该frame_id对应的page。 通过pages数组和frame_id获取该页面，如果该页是脏页，需要先写回磁盘，了解promise和future的使用方法。 page = pages_ + frame_id; if (page-\u003eIsDirty()) { // use promise and future to implement the communication between threads auto promise = disk_scheduler_-\u003eCreatePromise(); auto future = promise.get_future(); disk_scheduler_-\u003eSchedule({true, page-\u003eGetData(), page-\u003eGetPageId(), std::move(promise)}); // wait till promise is fulfilled future.get(); page-\u003eis_dirty_ = false; } 用AllocatePage()分配一个页面id，执行一系列初始化，返回页面。 page-\u003epage_id_ = AllocatePage(); page-\u003eResetMemory(); page-\u003epin_count_ = 1; page_table_[page-\u003epage_id_] = frame_id; replacer_-\u003eSetEvictable(frame_id, false); replacer_-\u003eRecordAccess(frame_id); *page_id = page-\u003epage_id_; ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:1","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"FetchPage(page_id_t page_id) 对于FetchPage ，如果空闲列表中没有可用页面且所有其他页面当前均已固定，则应返回 nullptr。 获取latch，如果page_id等于INVALID_PAGE_ID，返回nullptr。 如果在page_table_中找到了该page_id，则取出对应的frame_id，然后用pages数组得到page。replacer_记录它的访问、置为不可驱逐、pin加一。返回该page。 若是没有找到，则要通过free_list_或者驱逐来获得一个frame_id，同理NewPage()的时候也要先执行写回磁盘。 修改page_table_，同理NewPage()执行一系列初始化，然后利用disk_scheduler_修改从磁盘读取物理页面，存入page后返回。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:2","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"UnpinPage(page_id_t page_id, bool is_dirty) 对于UnpinPage ， is_dirty 参数跟踪页面在固定时是否被修改。 获取latch，如果page_table_中没找到，返回false。 拿到page，若是pin_count_以及为0，返回false。 pin_count_减1，若减完之后为0，置为可驱逐，然后注意is_dirty_设置为is_dirty || page-\u003eis_dirty_。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:3","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"FlushPage(page_id_t page_id) FlushPage应该刷新页面，无论其 pin 状态如何。 获取latch，page_id不合法或者没在page_table_中没找到，返回false。 拿到page，用disk_scheduler_写回磁盘。is_dirty_置为false。 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:4","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"DeletePage(page_id_t page_id) DeallocatePage()方法是一个空操作，它模拟释放磁盘上的页面，您应该在DeletePage()实现中调用它。 获取latch，若page_id不合法，返回false。若是在page_table_中没找到，先执行DeallocatePage(page_id)，返回true。 拿到page，如果pin_count_不为0，返回false。 执行一系列删除相关操作后DeallocatePage(page_id)。 page_table_.erase(page_id); replacer_-\u003eRemove(frame_id); free_list_.emplace_back(frame_id); page-\u003eResetMemory(); page-\u003epage_id_ = INVALID_PAGE_ID; page-\u003eis_dirty_ = false; page-\u003epin_count_ = 0; DeallocatePage(page_id); ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:5","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"FlushAllPages() 写回所有页面到磁盘 ","date":"2024-12-09","objectID":"/cmu15445-buffer_pool/:4:6","tags":["database"],"title":"Cmu15445 Buffer Pool","uri":"/cmu15445-buffer_pool/"},{"categories":["CMU"],"content":"Extendible Hash Index 在此编程项目中，将使用可扩展哈希的变体作为哈希方案，在数据库系统中实现磁盘支持的哈希索引。 下图显示了一个可扩展哈希表，其中header页最大深度为 2，directory页最大深度为 2，存储bucket页最多包含两个条目。值被省略，并且键的哈希值显示在bucket页面中，而不是键本身。 该索引提供快速数据检索，无需搜索数据库表中的每一行，从而实现快速随机查找。实现应该支持线程安全的搜索、插入和删除（包括增长/收缩目录和拆分/合并桶）。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:0:0","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Read/Write Page Guards 在Buffer Pool Manager中， FetchPage和NewPage函数返回指向已固定页面的指针。固定机制确保页面不会被逐出，直到页面上不再有任何读写操作。为了表明内存中不再需要该页，程序员必须手动调用UnpinPage。 另一方面，如果程序员忘记调用UnpinPage ，该页将永远不会被从缓冲池中逐出。由于缓冲池实际上以较少数量的帧运行，因此将有更多的页面进出磁盘的交换。不仅性能受到影响，而且错误也很难被发现。 所以第一个任务将实现BasicPageGuard ，它存储指向BufferPoolManager和Page对象的指针。页面防护确保一旦超出范围，就会在相应的Page对象上调用UnpinPage 。请注意，它仍然应该公开一个方法，供程序员手动取消固定页面。 由于BasicPageGuard隐藏了底层Page指针，因此它还可以提供只读/写入数据 API，这些 API 提供编译时检查，以确保为每个用例正确设置is_dirty标志。 在Page类中，有用于多线程保护的latch方法。与取消固定页面类似，程序员可能会忘记在使用页面后解锁页面。为了缓解这个问题，您将实现ReadPageGuard和WritePageGuard一旦页面超出范围，它们就会自动解锁页面。 需要为所有BasicPageGuard 、 ReadPageGuard和WritePageGuard实现以下函数。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:0","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"PageGuard(PageGuard \u0026\u0026that) 移动构造函数。拷贝构造函数和拷贝赋值的目的是将一个对象复制到另一个对象，而移动构造函数和移动赋值的目的是将资源的所有权从一个对象转移到另一个对象(这通常比复制成本低得多)。实现拷贝语义时需要使用const类型的左值引用作为形参，而实现移动语义时，需要使用非const的右值形参。 转移资源所有权（之后释放原来对象对资源的管理）。 BasicPageGuard::BasicPageGuard(BasicPageGuard \u0026\u0026that) noexcept : bpm_(that.bpm_), page_(that.page_), is_dirty_(that.is_dirty_) { that.page_ = nullptr; that.bpm_ = nullptr; that.is_dirty_ = false; } ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:1","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"operator=(PageGuard \u0026\u0026that) 移动赋值运算符。 自我赋值检查。 释放被赋值对象当前持有的资源。 转移资源所有权。 auto BasicPageGuard::operator=(BasicPageGuard \u0026\u0026that) noexcept -\u003e BasicPageGuard \u0026 { if (\u0026that == this) { return *this; } Drop(); bpm_ = that.bpm_; page_ = that.page_; is_dirty_ = that.is_dirty_; that.page_ = nullptr; that.bpm_ = nullptr; that.is_dirty_ = false; return *this; } ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:2","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Drop() Unpin and/or unlatch. 如果bpm_和page_不为nullptr，执行bpm_-\u003eUnpinPage。 bpm_和page_置为nullptr。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:3","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"~PageGuard() 析构函数。 调用Drop()。 还需要为BasicPageGuard实现以下升级功能。这些函数需要保证受保护的页面在升级过程中不会被从缓冲池中逐出。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:4","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"UpgradeRead() 升级到ReadPageGuard。 如果page_不等于nullptr，上读锁RLatch()。 当前资源所有权转移给ReadPageGuard，将它返回。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:5","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"UpgradeWrite() 升级到WritePageGuard。 如果page_不等于nullptr，上写锁WLatch()。 当前资源所有权转移给WritePageGuard，将它返回。 使用新的页面防护，在BufferPoolManager中实现以下包装器。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:6","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"FetchPageBasic(page_id_t page_id) 调用FetchPage获取page，用this和page构造BasicPageGuard返回。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:7","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"FetchPageRead(page_id_t page_id) 调用FetchPage获取page，若不为nullptr则上读锁RLatch()。 用this和page构造ReadPageGuard返回。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:8","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"FetchPageWrite(page_id_t page_id) 调用FetchPage获取page，若不为nullptr则上写锁WLatch()。 用this和page构造WritePageGuard返回。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:9","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"NewPageGuarded(page_id_t *page_id) 调用NewPage获取page，用this和page构造BasicPageGuard返回。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:1:10","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Extendible Hash Table Pages 每个可扩展哈希表头/目录/桶页对应于缓冲池取出的内存页的内容（即data_部分）。每次读取或写入页面时，必须首先从缓冲池中获取页面（使用其唯一的page_id ），重新解释将其转换为相应的类型，并在读取或写入页面后取消固定该页面。也就是利用任务1的PageGuard API来实现这个目标。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:2:0","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Header Page 标头页位于基于磁盘的可扩展哈希表的第一级，并且哈希表只有一个标头页。它存储指向目录页面的逻辑子指针。你可以把它想象成一个静态的一级目录页面。它有两个字段： directory_page_ids_：目录页面 id 的数组 bucket_size_：标题页可以处理的最大深度，也就是最开始图中的header(2)。 需要实现： Init(uint32_t max_depth)：赋值max_depth_，directory_page_ids_数组中的值都置为INVALID_PAGE_ID。 HashToDirectoryIndex(uint32_t hash)：把hash右移32-max_depth_位，保留最高max_depth_位返回。 GetDirectoryPageId(uint32_t directory_idx) SetDirectoryPageId(uint32_t directory_idx, page_id_t directory_page_id) MaxSize()：左移max_depth_位，2的max_depth次方。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:2:1","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Directory Page 目录页位于基于磁盘的可扩展哈希表的第二层。它们中的每一个都存储指向桶页面的逻辑子指针，以及用于处理桶映射和动态目录增长和收缩的元数据。目录页面有以下字段： max_depth_：标题页可以处理的最大深度，对应图中directory(2/2)。 global_depth_：当前目录全局深度，对应图中directory(2/2)。 local_depths_：桶页面局部深度的数组，对应图中directory下面表格的第二列，表示最后几位相同就在同一个桶。例如，图中00和10对应的局部深度为1，他俩最后一位相同，所以在同一个桶，但01和11的局部深度为2，他俩就不在同一个桶。 bucket_page_ids_：桶页面id的数组 部分需要实现的： Init(uint32_t max_depth)：赋值max_depth_，local_depths_数组中的值都值为0，bucket_page_ids_数组中的值都置为INVALID_PAGE_ID。 HashToBucketIndex(uint32_t hash)：用与运算取hash最后global_depth_位。 GetSplitImageIndex(uint32_t bucket_idx)：可以理解为计算兄弟桶的id，将该bucket_id的最高位反转得到的值返回。 return bucket_idx + (1 \u003c\u003c (global_depth_ - 1)); IncrGlobalDepth()：首先如果已经等于max_depth_，直接return。将目录增大小扩大一倍，操作相当于把现在的内容复制一遍给新增的那部分： void ExtendibleHTableDirectoryPage::IncrGlobalDepth() { if (global_depth_ \u003e= max_depth_) { return; } // double the size of the directory for (int i = 0; i \u003c 1 \u003c\u003c global_depth_; i++) { bucket_page_ids_[(1 \u003c\u003c global_depth_) + i] = bucket_page_ids_[i]; local_depths_[(1 \u003c\u003c global_depth_) + i] = local_depths_[i]; } global_depth_++; } DecrGlobalDepth()：若已经等于0，直接return。否则减一。 CanShrink()：GD等于零返回false。检查所有LD \u003c GD。 IncrLocalDepth和DecrLocalDepth：检查大小后直接增或减。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:2:2","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Bucket Page 桶页面位于基于磁盘的可扩展哈希表的第三层。它们是实际存储键值对的。有以下字段： size_：桶中保存的键值对的数量。 max_size_：桶可以处理的最大键值对数量。 array_：大小为桶页面局部深度的数组，存储键值对数据。 部分需要实现的： Lookup(const K \u0026key, V \u0026value, const KC \u0026cmp)：查找key，如果找到了把key(array[i],first)对应的值(array[i].second)赋给value，返回true。 Insert(const K \u0026key, const V \u0026value, const KC \u0026cmp)：查找key，如果已经存在，返回false。如果key不存在，则插入到数组最后，size增加，返回true。 Remove(const K \u0026key, const KC \u0026cmp)：查找key，找到了就array_中后面的元素往前移一位，size减少，返回true。没找到返回false。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:2:3","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Extendible Hashing Implementation 实现对插入、搜索和删除的支持。要求要实现桶拆分/合并和目录增长/收缩。先了解一下实现关键点： Empty Table：第一次创建一个空哈希表时，它应该只有一个唯一的Header Page，Directory pages 和 bucket pages按需创建。 Header Indexing：通过前面的部分应该已经明白了，使用最高有效位来索引标题页中的directory_page_ids_数组。 Directory Indexing：使用最低有效位来索引目录页中的bucket_page_ids_数组。 Bucket Splitting：如果没有空间插入，则必须分裂桶。 Bucket Merging：当桶变空的时候必须尝试合并。有一些方法可以通过检查桶及其分割映像的占用情况来更积极地进行合并，但这些昂贵的检查和额外的合并可能会增加抖动。这里为了使事情相对简单，用以下规则： 只能合并空桶。 仅当其分割图像具有相同的局部深度时，桶才能与其分割图像合并。 如果合并桶的新分割镜像为空，则应继续递归合并。 Directory Growing：意思就是目录会不断增长。 Directory Shrinking：仅当每个桶的局部深度严格小于目录的全局深度时才收缩目录。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:3:0","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"DiskExtendibleHashTable 构造函数 主要是把传入的参数都赋值，然后用NewPageGuarded创建一个page guard，再用模版函数转换为ExtendibleHTableHeaderPage，执行init。 auto header_guard = bpm-\u003eNewPageGuarded(\u0026header_page_id_); auto header_page = header_guard.template AsMut\u003cExtendibleHTableHeaderPage\u003e(); header_page-\u003eInit(header_max_depth_); ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:3:1","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"GetValue auto DiskExtendibleHashTable\u003cK, V, KC\u003e::GetValue(const K \u0026key, std::vector *result, Transaction *transaction) const -\u003e bool 从标头页开始搜索：利用FetchPageRead获取header_guard，同样的方式转换为header_page，用Hash函数处理key得到hash值，然后用HashToDirectoryIndex处理得到dir_index，即可找到目录页。这里就可以Drop掉header_guard了。 若得到的目录页的PageId为INVALID_PAGE_ID，则返回false。否则获取该目录页，HashToBucketIndex得到桶的索引。接着等到成功获取到桶页面切不为INVALID_PAGE_ID的时候再Drop掉dir_guard。 最后就可以在桶里找了，用桶的LookUp函数搜索，找到了就push_back到result中，Drop掉bucket_guard，返回true。没找到就返回false。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:3:2","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"SplitBucket auto DiskExtendibleHashTable\u003cK, V, KC\u003e::SplitBucket(ExtendibleHTableDirectoryPage *directory, ExtendibleHTableBucketPage\u003cK, V, KC\u003e *bucket, uint32_t bucket_idx) -\u003e bool New一个新的页面作为新的桶页，并升级为WritePageGuard，如果得到的PageId为INVALID_PAGE_ID，说明分配失败，直接return false。 WritePageGuard split_bucket_guard = bpm_-\u003eNewPageGuarded(\u0026split_page_id).UpgradeWrite(); 得到分裂的新桶page，执行Init，因为它就是传入的那个bucket（执行分裂前该桶已经增加过LD了）的兄弟，所以它在目录中的索引就是GetSplitImageIndex(bucket_idx)，LD也与之相同，所以目录需要对新桶执行SetBucketPageId和SetLocalDepth。 由于桶的分裂，LD增加，使得之前存放的第一个桶里的内容需要拿出来重新分配到这两个桶里。用一个vector记录下第一个桶里的所有键值对，然后清空该桶，再根据每个键的最低有效位来选择放到哪个桶里。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:3:3","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Insert auto DiskExtendibleHashTable\u003cK, V, KC\u003e::Insert(const K \u0026key, const V \u0026value, Transaction *transaction) -\u003e bool 先执行GetValue(key, \u0026v, transaction)，若找到了，说明键已存在，直接返回false。 同样从标头页开始找，区别就是这里用的是FetchPageWrite，因为涉及到写。通过hash_key在标头页中得到dir_page_id。 如果dir_page_id等于INVALID_PAGE_ID，说明目录页不存在，需要新分配目录页，并初始化，我们另起一个函数InsertToNewDirectory来处理，在下面给出。否则，Drop掉header_guard，获取目录页，根据目录页和hash_key得到桶的bucket_page_id。因为目录页之后会写，不着急Drop。 同理，如果bucket_page_id等于INVALID_PAGE_ID，另起一个函数InsertToNewBucket来处理。否则，拿到桶页，用桶页的Insert函数插入，如果成功就返回true。 如果失败说明桶已满，需要分裂桶。先检查LD已经等于GD，如果是，再检查GD是否大于等于MD，如果不是，可以IncrGlobalDepth，如果也是，说明满满的了，分不了，返回false。然后就可以增加这个桶的LD了，同时增加它兄弟的LD(例如最开始图中的00和10，要加一起加)。 执行到这里，就可以开始分裂了SplitBucket，分裂失败就返回false，否则Drop掉bucket_guard和dir_guard，然后递归return Insert(..)。 // 这里两个函数其实主要是在新建有效页，让对应的索引指向有效页面，最后还是要调用Insert来重新插入。 template \u003ctypename K, typename V, typename KC\u003e auto DiskExtendibleHashTable\u003cK, V, KC\u003e::InsertToNewDirectory(ExtendibleHTableHeaderPage *header, uint32_t directory_idx, uint32_t hash, const K \u0026key, const V \u0026value) -\u003e bool { page_id_t dir_page_id = INVALID_PAGE_ID; // allocate a new directory page WritePageGuard dir_guard = bpm_-\u003eNewPageGuarded(\u0026dir_page_id).UpgradeWrite(); auto dir_page = dir_guard.AsMut\u003cExtendibleHTableDirectoryPage\u003e(); dir_page-\u003eInit(directory_max_depth_); header-\u003eSetDirectoryPageId(directory_idx, dir_page_id); auto bucket_idx = dir_page-\u003eHashToBucketIndex(hash); return InsertToNewBucket(dir_page, bucket_idx, key, value); } template \u003ctypename K, typename V, typename KC\u003e auto DiskExtendibleHashTable\u003cK, V, KC\u003e::InsertToNewBucket(ExtendibleHTableDirectoryPage *directory, uint32_t bucket_idx, const K \u0026key, const V \u0026value) -\u003e bool { page_id_t bucket_page_id = INVALID_PAGE_ID; // allocate a new bucket page WritePageGuard bucket_guard = bpm_-\u003eNewPageGuarded(\u0026bucket_page_id).UpgradeWrite(); auto bucket_page = bucket_guard.AsMut\u003cExtendibleHTableBucketPage\u003cK, V, KC\u003e\u003e(); bucket_page-\u003eInit(bucket_max_size_); directory-\u003eSetBucketPageId(bucket_idx, bucket_page_id); return bucket_page-\u003eInsert(key, value, cmp_); } ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:3:4","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Remove auto DiskExtendibleHashTable\u003cK, V, KC\u003e::Remove(const K \u0026key, Transaction *transaction) -\u003e bool 同样用FetchPageWrite获取标头页，然后根据hash_key得到目录页的dir_page_id，Drop掉header_guard。如果得到的是INVALID_PAGE_ID，说明目录页不存在，直接return false。 一样的操作得到桶页，如果桶页PageId为INVALID_PAGE_ID，直接return false。否则用桶页执行Remove，此时Drop掉bucket_guard。如果Remove失败，返回false。 如果Remove成功，开始检测是否需要合并。一个大循环while(LD \u003e 0)，内部先获取该桶的兄弟桶，然后判断如果俩兄弟LD不一样 or 两个都不是空桶，直接break出来。否则，这轮循环需要合并：bpm_-\u003eDeletePage掉那个空桶，LD减1，更新目录。 上一个循环结束后，合并完成。这里再一个循环，判断是否CanShrink()，如果可以就直接目录DecrGlobalDepth()，直到不能缩为止。Drop掉dir_guard。返回true。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:3:5","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Concurrency Control 多线程并发控制。注意实现过程中Fetch和Drop的时机。 ","date":"2024-12-01","objectID":"/cmu15445-extendible_hash_index/:4:0","tags":["database"],"title":"Cmu15445 Extendible Hash Index","uri":"/cmu15445-extendible_hash_index/"},{"categories":["CMU"],"content":"Trie trie 实现的是键值存储，字符串键可以映射到任何类型值。键的值存储在改键的最后一个字符的终端节点。例如将（“ab”， 1） 和 （“ac”， “val”） 插入到 trie 中。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:0:0","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"copy-on-write 写时复制，任何操作都不会修改原先的trie节点，而是会为修改后的数据创建新节点，并为修改后的trie返回新的根节点。这样做可以很方便的访问旧的trie，撤销操作也容易。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:1:0","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"插入 例如对之前的trie插入（“ad”， 2），这里会复用原先树中的两个字节点，并创建一个新的值节点2，然后用他们三个来创建新的Node2。 然后插入（“b”， 3），创建一个新的root、一个新的值节点3并复用之前的节点。这样操作前后的内容，只要有root，就可以访问当时的完整数据。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:1:1","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"删除 父节点也可以有值，例如上述trie可以插入（“a”， “abc”）。删除的时候先删除对应的节点，之后要注意的是，需要清除所有不必要的节点（即没有值且没有子节点的节点）。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:1:2","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"implement Get(key) 获取 key 对应的值。 遍历key的每一个字符，从当前root开始，通过查看每个字符是否在current-\u003echildren_中，没找到或者current为空直接返回nullptr，找到了就把current更新为current-\u003echildren_.at(ch)。 经过上一步骤之后，现在current就是key对应的值节点。然后利用dynamic_cast把它转换为const TrieNodeWithValue\u003cT\u003e *。若是转换后为nullptr，说明类型不匹配，直接返回nullptr，否则返回该节点。 auto *node{dynamic_cast\u003cconst TrieNodeWithValue\u003cT\u003e *\u003e(current.get())}; Put(key, value) 为 key 设置对应的值。如果键已存在，则覆盖现有值。请注意，值的类型可能是不可复制的（即 std：：unique_ptr）。此方法返回一个新的 trie。 如果key.empty()。判断root是否有孩子，若有孩子，用孩子和传入的value创建一个new_root返回，否则直接用value创建new_root返回。 if (root_-\u003echildren_.empty()) { new_root = std::make_unique\u003cTrieNodeWithValue\u003cT\u003e\u003e(std::move(val_p)); } else { new_root = std::make_unique\u003cTrieNodeWithValue\u003cT\u003e\u003e(root_-\u003echildren_, std::move(val_p)); } 利用节点的Clone()函数，从root开始克隆，先判断root是否存在，若不存在则用std::make_shared\u003cTrieNode\u003e()新建一个TrieNode。该节点即为一会要返回的新的root。 开始遍历key。每次遍历，先判断若当前字符不是key的最后一个字符，进行下述操作：在当前节点clone_current的孩子中找，若找到了则把这个孩子Clone()一份作为clone_current的新孩子，然后更新clone_current为这个孩子，继续往下找。若没找到该孩子，则创建一个新的TrieNode。 若是遍历到最后一个字符，判断是否有对应的孩子，同理1，若有则用这个孩子的children_和传入的value创建一个TrieNodeWithValue，若没有就直接用value创建。建好了之后连上当前的clone_current-\u003echildren_[ch] = new_child就大功告成。返回新的root。 Remove(key) 删除 key 的值。此方法返回一个新的 trie。 还是先处理边界情况，若!root_直接返回*this。若key.empty()：先克隆当前root，有孩子就用孩子创建一个无值节点返回，没孩子直接返回nullptr。 开始遍历key，任何一个节点没找到则直接返回*this，并把路上经过的每一个节点克隆一遍放入一个vector（或者直接用stack）中，并前后连接上。遍历完后把最后一个节点的is_value_node置为false。 // clone every node in key into vec. clone_current = std::shared_ptr\u003cTrieNode\u003e(current-\u003eClone()); vec.back()-\u003echildren_[ch] = clone_current; vec.push_back(clone_current); 自底向上删除，从vector的最后一个节点往前。若!is_value_node：判断若没有孩子，则erase掉该节点，否则用孩子创建一个TrieNode的无值节点替换。 判断新root是否为空且!is_value_node，若满足则返回nullptr。否则返回新root。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:1:3","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"注意 所有操作都不会在原始trie上修改，应该是创建一个新的trie节点并尽可能复用旧的trie节点。 创建新节点时将其转换为智能指针，复用节点时可以复制std::shared_ptr\u003cTrieNode\u003e，智能共享指针的增加不会复制底层数据，且会在没有人引用底层对象时自动释放对象。 std::move()可以把左值转换为右值。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:1:4","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"Concurrent Key-Value Store ","date":"2024-11-17","objectID":"/cmu15445-trie/:2:0","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"Triestore 在拥有可在单线程环境中使用的写入时复制trie后，为多线程环境实现并发键值存储。并发键值存储应同时为多个读取器和单个写入器提供服务。 此外，如果我们从trie获得对值的引用，那么无论我们如何修改 trie，我们都应该能够访问它。Trie的Get函数仅返回一个指针。如果存储此值的trie节点已被删除，则指针将悬空。因此，在 TrieStore 中，我们返回一个ValueGuard，它存储对值的引用和对应于 trie 结构根的TrieNode，以便在我们存储ValueGuard时可以访问该值。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:2:1","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"implement Get(key) 拿到root lock，获取root，然后释放root lock。注意在持有root lock的时候，先不要查找trie里的值。 查找trie里的值。 如果找到值了，返回一个ValueGuard对象，它持有对值的引用和对应的root，否则返回std::nullopt。 Put(key, value) 获取write_lock_，执行put。 获取root lock，修改root_为新的root。 Remove(key) 获取write_lock_，执行remove。 获取root lock，修改root_为新的root。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:2:2","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"SQL String Functions 需要实现上层和下层SQL函数。这可以分2个步骤完成： 在 string_expression.h 中实现函数逻辑。 在 BusTub 中注册函数，以便 SQL 框架可以在用户执行 SQL 时以 plan_func_call.cpp 调用你的函数。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:3:0","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"实现函数逻辑 简单的大小写转换，利用std::toupper和std::tolower。 ","date":"2024-11-17","objectID":"/cmu15445-trie/:3:1","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"},{"categories":["CMU"],"content":"注册函数 auto Planner::GetFuncCallFromFactory(const std::string \u0026func_name, std::vector\u003cAbstractExpressionRef\u003e args) -\u003e AbstractExpressionRef { // 1. check if the parsed function name is \"lower\" or \"upper\". // 2. verify the number of args (should be 1), refer to the test cases for when you should throw an `Exception`. // 3. return a `StringExpression` std::shared_ptr. if ((func_name == \"lower\" || func_name == \"upper\") \u0026\u0026 args.size() == 1) { return static_cast\u003cstd::shared_ptr\u003cStringExpression\u003e\u003e(std::make_shared\u003cStringExpression\u003e( args[0], func_name == \"lower\" ? StringExpressionType::Lower : StringExpressionType::Upper)); } throw Exception(fmt::format(\"func call {} not supported in planner yet\", func_name)); } ","date":"2024-11-17","objectID":"/cmu15445-trie/:3:2","tags":["database"],"title":"Cmu15445 Trie","uri":"/cmu15445-trie/"}]